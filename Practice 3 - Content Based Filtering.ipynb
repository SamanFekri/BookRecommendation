{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2020/21\n",
    "\n",
    "### Practice 3 - Content Based recommenders\n",
    "\n",
    "\n",
    "### Load the data you saw last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab1164d0871f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0murlretrieve\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"http://files.grouplens.org/datasets/movielens/ml-10m.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdataFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mURM_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ml-10M100K/ratings.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/Movielens_10M\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mURM_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURM_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import zipfile, os\n",
    "\n",
    "# If file exists, skip the download\n",
    "data_file_path = \"data/Movielens_10M/\"\n",
    "data_file_name = data_file_path + \"movielens_10m.zip\"\n",
    "\n",
    "# If directory does not exist, create\n",
    "if not os.path.exists(data_file_path):\n",
    "    os.makedirs(data_file_path)\n",
    "\n",
    "if not os.path.exists(data_file_name):\n",
    "    urlretrieve (\"http://files.grouplens.org/datasets/movielens/ml-10m.zip\", data_file_name)\n",
    "    \n",
    "dataFile = zipfile.ZipFile(data_file_name)\n",
    "URM_path = dataFile.extract(\"ml-10M100K/ratings.dat\", path=\"data/Movielens_10M\")\n",
    "URM_file = open(URM_path, 'r')\n",
    "\n",
    "\n",
    "def rowSplit (rowString):\n",
    "    \n",
    "    split = rowString.split(\"::\")\n",
    "    split[3] = split[3].replace(\"\\n\",\"\")\n",
    "    \n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = float(split[2])\n",
    "    split[3] = int(split[3])\n",
    "    \n",
    "    result = tuple(split)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "URM_file.seek(0)\n",
    "URM_tuples = []\n",
    "\n",
    "for line in URM_file:\n",
    "   URM_tuples.append(rowSplit (line))\n",
    "\n",
    "userList, itemList, ratingList, timestampList = zip(*URM_tuples)\n",
    "\n",
    "userList = list(userList)\n",
    "itemList = list(itemList)\n",
    "ratingList = list(ratingList)\n",
    "timestampList = list(timestampList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_ID_stats(ID_list, label):\n",
    "    \n",
    "    min_val = min(ID_list)\n",
    "    max_val = max(ID_list)\n",
    "    unique_val = len(set(ID_list))\n",
    "    missing_val = 1 - unique_val/(max_val - min_val)\n",
    "\n",
    "    print(\"{} data, ID: min {}, max {}, unique {}, missig {:.2f} %\".format(label, min_val, max_val, unique_val, missing_val*100))\n",
    "\n",
    "    \n",
    "    \n",
    "list_ID_stats(userList, \"User\")\n",
    "list_ID_stats(itemList, \"Item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all = URM_all.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For items in particular most have no interactions. Sometimes it may be better to remove them to avoid creating big data structures with no need. In this case empty columns will nave no impact and we leave them as is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now load the content informations in the same way:\n",
    "## In this case we are using tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_path = dataFile.extract(\"ml-10M100K/tags.dat\", path = \"data/Movielens_10M\")\n",
    "ICM_file = open(ICM_path, 'r')\n",
    "\n",
    "def rowSplit (rowString):\n",
    "    split = rowString.split(\"::\")\n",
    "    split[3] = split[3].replace(\"\\n\",\"\")\n",
    "    \n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = str(split[2]) # tag is a string, not a float like the rating\n",
    "    split[3] = int(split[3])\n",
    "    \n",
    "    result = tuple(split)\n",
    "    \n",
    "    return result\n",
    "\n",
    "ICM_file.seek(0)\n",
    "ICM_tuples = []\n",
    "\n",
    "for line in ICM_file:\n",
    "    ICM_tuples.append(rowSplit(line))\n",
    "    \n",
    "userList_icm, itemList_icm, tagList_icm, timestampList_icm = zip(*ICM_tuples)\n",
    "\n",
    "userList_icm = list(userList_icm)\n",
    "itemList_icm = list(itemList_icm)\n",
    "tagList_icm = list(tagList_icm)\n",
    "timestampList_icm = list(timestampList_icm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ID_stats(userList_icm, \"Users ICM\")\n",
    "list_ID_stats(itemList_icm, \"Items ICM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that most users and items have no data associated to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTags = len(set(tagList_icm))\n",
    "\n",
    "print (\"Number of tags\\t {}, Number of item-tag tuples {}\".format(numTags, len(tagList_icm)))\n",
    "\n",
    "print(\"\\nData example:\")\n",
    "print(userList_icm[0:10])\n",
    "print(itemList_icm[0:10])\n",
    "print(tagList_icm[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The numbers of items and users in the ICM matrix is different from what we saw in the URM, why?\n",
    "\n",
    "### The tags are string, we should traslate them into numbers so we can use them as indices in the ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "ICM_label_encoder = preprocessing.LabelEncoder()\n",
    "ICM_label_encoder.fit(tagList_icm)\n",
    "\n",
    "tagList_icm = ICM_label_encoder.transform(tagList_icm)\n",
    "\n",
    "print(tagList_icm[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now build the ICM\n",
    "\n",
    "#### Be careful with the indices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_items = URM_all.shape[1]\n",
    "n_tags = max(tagList_icm) + 1\n",
    "\n",
    "ICM_shape = (n_items, n_tags)\n",
    "\n",
    "ones = np.ones(len(tagList_icm))\n",
    "ICM_all = sps.coo_matrix((ones, (itemList_icm, tagList_icm)), shape = ICM_shape)\n",
    "ICM_all = ICM_all.tocsr()\n",
    "\n",
    "ICM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at the ICM\n",
    "\n",
    "### We leverage CSR and CSC indptr data structure to compute the number of cells that have values for that row or column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_all = sps.csr_matrix(ICM_all)\n",
    "features_per_item = np.ediff1d(ICM_all.indptr)\n",
    "\n",
    "ICM_all = sps.csc_matrix(ICM_all)\n",
    "items_per_feature = np.ediff1d(ICM_all.indptr)\n",
    "\n",
    "ICM_all = sps.csr_matrix(ICM_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_per_item.shape)\n",
    "print(items_per_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_item = np.sort(features_per_item)\n",
    "items_per_feature = np.sort(items_per_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline  \n",
    "\n",
    "pyplot.plot(features_per_item, 'ro')\n",
    "pyplot.ylabel('Num features ')\n",
    "pyplot.xlabel('Item Index')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(items_per_feature, 'ro')\n",
    "pyplot.ylabel('Num items ')\n",
    "pyplot.xlabel('Feature Index')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now build the recommender algorithm, but first we need the train/test split and the evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.80)\n",
    "URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train, train_percentage = 0.80)\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base.Similarity.Compute_Similarity_Python import Compute_Similarity_Python\n",
    "\n",
    "class ItemKNNCBFRecommender(object):\n",
    "    \n",
    "    def __init__(self, URM, ICM):\n",
    "        self.URM = URM\n",
    "        self.ICM = ICM\n",
    "        \n",
    "            \n",
    "    def fit(self, topK=50, shrink=100, normalize = True, similarity = \"cosine\"):\n",
    "        \n",
    "        similarity_object = Compute_Similarity_Python(self.ICM.T, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "        \n",
    "        self.W_sparse = similarity_object.compute_similarity()\n",
    "\n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to define Cosine similarity... Let's look at the attached source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also a [list of commonly used KNN similarity heuristics](https://github.com/MaurizioFD/RecSys_Course_2018/blob/master/slides/List_of_KNN_similarity_heuristics.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A KNN is built with the following steps:\n",
    "* Compute the similarity of an item with all others\n",
    "* Select the k-highest similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 50\n",
    "shrink = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerator is the dot product of the item features times the whole ICM data transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_vector = ICM_all[item_id].dot(ICM_all.T).toarray().ravel()\n",
    "numerator_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_norms = np.sqrt(np.array(ICM_all.T.power(2).sum(axis=0))).ravel()\n",
    "item_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator will be the product of norms plus the srink term and a small value which prevents the denominator to be zero (only for non-negative data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator_vector = item_norms[item_id] * item_norms + shrink + 1e-6\n",
    "denominator_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_vector = numerator_vector/denominator_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the similarity from the highest to the lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_item_indices = np.argsort(-similarity_vector)\n",
    "sorted_item_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(similarity_vector[sorted_item_indices], 'ro')\n",
    "pyplot.ylabel('Similarity')\n",
    "pyplot.xlabel('Item')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select the k most similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(similarity_vector[sorted_item_indices[0:k]], 'ro')\n",
    "pyplot.ylabel('Similarity')\n",
    "pyplot.xlabel('Item')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation is performed for all items.\n",
    "A simple strategy to perform it efficiently is to vectorize the most computationally intensive part, the dot product, on a group of items. The speedup can be of a factor of 10-100.\n",
    "This strategy is limited by the fact that the result of the dot product is a huge item-item dense similarity which likely does not fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "try:\n",
    "    numerator_matrix = ICM_all.dot(ICM_all.T).toarray()\n",
    "    \n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is:\n",
    "* Compute the numerator a block of items at a time leveraging vectorization while not running out of memory\n",
    "* Extract the k-nn on those items\n",
    "* Built incrementally the sparse similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 100\n",
    "\n",
    "numerator_block = ICM_all[0:block_size].dot(ICM_all.T).toarray()\n",
    "numerator_block.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the speed to compute the dot product on the whole similarity of the two strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_items = ICM_all.shape[0]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for n_item in range(n_items):\n",
    "    numerator_vector = ICM_all[item_id].dot(ICM_all.T).toarray().ravel()\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Computing the similarity one item at a time runs at {:.2f} items/sec\".format(n_items/(end_time-start_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = ICM_all.shape[0]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "block_size = 100\n",
    "blocks_start_positions = range(0, n_items, block_size)\n",
    "\n",
    "for start_pos in blocks_start_positions:\n",
    "    end_pos = min(start_pos + block_size, n_items)\n",
    "    \n",
    "    numerator_block = ICM_all[start_pos:end_pos].dot(ICM_all.T).toarray()\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Computing the similarity in blocks of 100 items at a time runs at {:.2f} items/sec\".format(n_items/(end_time-start_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this case the vectorized implementation runs >50 times faster!\n",
    "\n",
    "#### Usually most of the speed gain comes with blocks of 100 or so items, depending on the system. Much higher than that tends to not be beneficial while requiring increasingly more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an example of something you should *never* do, nested loops to compute the similarity of each item without vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for n_item in range(n_items):\n",
    "    for second_item in range(n_items):\n",
    "        numerator_vector = ICM_all[item_id].dot(ICM_all[second_item].T)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Computing the similarity with nested loops runs at {:.2f} items/sec\".format(n_items/(end_time-start_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see how incredibly slow nested loops are compared to a well vectorized implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our CBF recommender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = ItemKNNCBFRecommender(URM_train, ICM_all)\n",
    "recommender.fit(shrink=0.0, topK=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userList_unique = list(set(userList_icm))\n",
    "for user_id in userList_unique[0:10]:\n",
    "    print(recommender.recommend(user_id, at=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's talk about speed\n",
    "\n",
    "#### Time to compute recommendations for a fixed group of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_users_to_test = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for user_id in range(n_users_to_test):\n",
    "    recommender.recommend(user_id, at=5)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Reasonable implementation speed is {:.2f} usr/sec\".format(n_users_to_test/(end_time-start_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add a common mistake.... a CSC URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_train_csc = URM_train.tocsc()\n",
    "\n",
    "print(type(ICM_all))\n",
    "recommender = ItemKNNCBFRecommender(URM_train_csc, ICM_all)\n",
    "recommender.fit(shrink=0.0, topK=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, traceback\n",
    "\n",
    "try:\n",
    "\n",
    "    n_users_to_test = 1000\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for user_id in range(n_users_to_test):\n",
    "        recommender.recommend(user_id, at=5)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Wrong implementation speed is {:.2f} usr/sec\".format(n_users_to_test/(end_time-start_time)))\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "        \n",
    "    print(\"Exception {}\".format(str(e)))\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "\n",
    "#### Once we have built our model we can play with its parameters\n",
    "* Number of neighbors\n",
    "* Shrinkage\n",
    "* Similarity type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KNN.ItemKNNCBFRecommender import ItemKNNCBFRecommender\n",
    "\n",
    "x_tick = [10, 50, 100, 200, 500]\n",
    "MAP_per_k = []\n",
    "\n",
    "for topK in x_tick:\n",
    "    \n",
    "    print(type(URM_train))\n",
    "    recommender = ItemKNNCBFRecommender(URM_train, ICM_all)\n",
    "    recommender.fit(shrink=0.0, topK=topK)\n",
    "    \n",
    "    result_dict, _ = evaluator_test.evaluateRecommender(recommender)\n",
    "    \n",
    "    MAP_per_k.append(result_dict[10][\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(x_tick, MAP_per_k)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('TopK')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On this dataset the number of neighbors does not have a great impact on MAP. Higher values of TopK might work even better\n",
    "\n",
    "#### Different datasets will behave in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tick = [0, 10, 50, 100, 200, 500]\n",
    "MAP_per_shrinkage = []\n",
    "\n",
    "for shrink in x_tick:\n",
    "    \n",
    "    recommender = ItemKNNCBFRecommender(URM_train, ICM_all)\n",
    "    recommender.fit(shrink=shrink, topK=100)\n",
    "    \n",
    "    result_dict, _ = evaluator_test.evaluateRecommender(recommender)\n",
    "    \n",
    "    MAP_per_shrinkage.append(result_dict[10][\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(x_tick, MAP_per_shrinkage)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('Shrinkage')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The shrinkage value (i.e. support) have a much stronger impact. Combine a parameter search with the two to ensure maximum recommendation quality\n",
    "\n",
    "## Be careful, overfitting!\n",
    "\n",
    "#### While a thorough parameter tuning might result in significantly higher MAP on your validation split, it could have only marginally better or even worse MAP on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.link-assistant.com/images/news/tf-idf-tool-for-seo/screen-03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tot_items = ICM_all.shape[0]\n",
    "\n",
    "# let's count how many items have a certain feature\n",
    "items_per_feature = (ICM_all > 0).sum(axis=0)\n",
    "\n",
    "IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "\n",
    "print(ICM_all.shape)\n",
    "print(IDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(np.sort(IDF))\n",
    "pyplot.ylabel('IDF')\n",
    "pyplot.xlabel('Sorted feature')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest ranked features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = np.argsort(-IDF)\n",
    "\n",
    "highest_ranked_features = sorted_features[:20]\n",
    "ICM_label_encoder.inverse_transform(highest_ranked_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowest ranked features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_ranked_features = sorted_features[-20:]\n",
    "ICM_label_encoder.inverse_transform(lowest_ranked_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_idf = ICM_all.copy()\n",
    "# compute the number of non-zeros in each col\n",
    "# NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "col_nnz = np.diff(sps.csc_matrix(ICM_idf).indptr)\n",
    "print(col_nnz.shape)\n",
    "print(ICM_idf.shape)\n",
    "print(IDF.shape)\n",
    "# then normalize the values in each col\n",
    "ICM_idf.data *= np.repeat(IDF, col_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommender_idf = ItemKNNCBFRecommender(URM_train, ICM_idf)\n",
    "recommender_idf.fit(shrink=0.0, topK=50)\n",
    "\n",
    "result_dict, _ = evaluator_test.evaluateRecommender(recommender_idf)\n",
    "result_dict[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is  a small gain over the non-weighted ICM. Try other feature weighting methods like BM25..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base.IR_feature_weighting import okapi_BM_25\n",
    "\n",
    "ICM_BM25 = ICM_all.copy().astype(np.float32)\n",
    "ICM_BM25 = okapi_BM_25(ICM_BM25)\n",
    "ICM_BM25 = ICM_BM25.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommender_bm25 = ItemKNNCBFRecommender(URM_train, ICM_BM25)\n",
    "recommender_bm25.fit(shrink=0.0, topK=50)\n",
    "\n",
    "result_dict, _ = evaluator_test.evaluateRecommender(recommender_bm25)\n",
    "result_dict[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another small gain over TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unnormalized similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_dot = ItemKNNCBFRecommender(URM_train, ICM_all)\n",
    "recommender_dot.W_sparse = ICM_all * ICM_all.T\n",
    "\n",
    "result_dict, _ = evaluator_test.evaluateRecommender(recommender_dot)\n",
    "result_dict[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
