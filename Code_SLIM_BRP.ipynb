{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy.sparse as sps\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"./input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RM_train=pd.read_csv('./input/data_train.csv')\n",
    "R_test=pd.read_csv('./input/data_target_users_test.csv')\n",
    "URM=pd.read_csv('./input/data_train.csv')\n",
    "ICM = pd.read_csv('./input/data_ICM_title_abstract.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## URM all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_tuples = [tuple(x) for x in URM.to_numpy()]\n",
    "\n",
    "userList, itemList, ratingList = zip(*URM_tuples)\n",
    "\n",
    "userList = list(userList)\n",
    "userList=np.array(userList,dtype=np.int64)\n",
    "itemList = list(itemList)\n",
    "itemList=np.array(itemList,dtype=np.int64)\n",
    "\n",
    "ratingList = list(ratingList)                        #not needed\n",
    "ratingList=np.array(ratingList,dtype=np.int64)       #not needed\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all = URM_all.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items\t 24896, Number of users\t 7947\n",
      "Number of Intraction \t 113268\n",
      "Max ID items\t 25974, Max Id users\t 7946\n",
      "\n",
      "Average interactions per user 14.25\n",
      "Average interactions per item 4.55\n",
      "\n",
      "Sparsity 99.94 %\n"
     ]
    }
   ],
   "source": [
    "userList_unique = list(set(userList))\n",
    "itemList_unique = list(set(itemList))\n",
    "\n",
    "numUsers = len(userList_unique)\n",
    "numItems = len(itemList_unique)\n",
    "\n",
    "numberInteractions= len(URM_tuples)\n",
    "print (\"Number of items\\t {}, Number of users\\t {}\".format(numItems, numUsers))\n",
    "print(\"Number of Intraction \\t {}\" .format(numberInteractions))\n",
    "print (\"Max ID items\\t {}, Max Id users\\t {}\\n\".format(max(itemList_unique), max(userList_unique)))\n",
    "print (\"Average interactions per user {:.2f}\".format(numberInteractions/numUsers))\n",
    "print (\"Average interactions per item {:.2f}\\n\".format(numberInteractions/numItems))\n",
    "\n",
    "print (\"Sparsity {:.2f} %\".format((1-float(numberInteractions)/(numItems*numUsers))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ICM all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25975x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 490691 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ICM_tuples = [tuple(x) for x in ICM.to_numpy()]\n",
    "itemList_icm, featureList_icm, scoreList_icm = zip(*ICM_tuples)\n",
    "\n",
    "itemList_icm = list(itemList_icm)\n",
    "itemList_icm = np.array(itemList_icm,dtype=np.int64)\n",
    "\n",
    "featureList_icm = list(featureList_icm)\n",
    "featureList_icm = np.array(featureList_icm,dtype=np.int64)\n",
    "\n",
    "scoreList_icm = list(scoreList_icm)\n",
    "scoreList_icm = np.array(scoreList_icm,dtype=np.float64)\n",
    "\n",
    "ICM_all = sps.coo_matrix((scoreList_icm, (itemList_icm, featureList_icm)))\n",
    "\n",
    "ICM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userTestList = [x for x in R_test.to_numpy()]\n",
    "userTestList = zip(*userTestList)\n",
    "userTestList = [list(a) for a in userTestList][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 77 (0.97 %) of 7947 users have no train items\n",
      "Warning: 2284 (28.74 %) of 7947 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.80)\n",
    "# URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train, train_percentage = 0.80)\n",
    "\n",
    "# evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement SLIM BPR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Sampling\n",
    "\n",
    "#### Create a mask of positive interactions. How to build it depends on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7947x25975 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 90614 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_mask = URM_train.copy()\n",
    "URM_mask.data[URM_mask.data < 1] = 0\n",
    "\n",
    "URM_mask.eliminate_zeros()\n",
    "URM_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = URM_mask.shape[0]\n",
    "n_items = URM_mask.shape[1]\n",
    "\n",
    "\n",
    "# Extract users having at least one interaction to choose from\n",
    "eligibleUsers = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "    start_pos = URM_mask.indptr[user_id]\n",
    "    end_pos = URM_mask.indptr[user_id+1]\n",
    "    if len(URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "        eligibleUsers.append(user_id)\n",
    "                \n",
    "\n",
    "def sampleTriplet():\n",
    "    \n",
    "    # By randomly selecting a user in this way we could end up \n",
    "    # with a user with no interactions\n",
    "    #user_id = np.random.randint(0, n_users)\n",
    "    \n",
    "    user_id = np.random.choice(eligibleUsers)\n",
    "    \n",
    "    # Get user seen items and choose one\n",
    "    userSeenItems = URM_mask[user_id,:].indices\n",
    "    pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "    negItemSelected = False\n",
    "\n",
    "    # It's faster to just try again then to build a mapping of the non-seen items\n",
    "    while (not negItemSelected):\n",
    "        neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "        if (neg_item_id not in userSeenItems):\n",
    "            \n",
    "            negItemSelected = True\n",
    "\n",
    "    return user_id, pos_item_id, neg_item_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3670, 20897, 19396)\n",
      "(7538, 3971, 24663)\n",
      "(2826, 13196, 25151)\n",
      "(6431, 8245, 3943)\n",
      "(2446, 17202, 13575)\n",
      "(269, 13099, 16976)\n",
      "(1053, 20624, 10945)\n",
      "(6291, 9322, 1250)\n",
      "(3805, 6964, 10140)\n",
      "(7116, 1212, 7606)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampleTriplet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Computing prediction\n",
    "\n",
    "#### The prediction depends on the model: SLIM, Matrix Factorization... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to initialize our model. In case of SLIM it works best to initialize S as zero, in case of MF you cannot because of how the gradient is computed and you have to initialize at random. Here we initialize SLIM at random just so that we have some numbers to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.random.random((n_items,n_items))\n",
    "similarity_matrix[np.arange(n_items),np.arange(n_items)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, positive_item_id, negative_item_id = sampleTriplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1112"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11120"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1112,  4825,  9358, 19123, 23109, 24427, 25770], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userSeenItems = URM_mask[user_id,:].indices\n",
    "userSeenItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i is 3.02, x_j is 4.34\n"
     ]
    }
   ],
   "source": [
    "x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "print(\"x_i is {:.2f}, x_j is {:.2f}\".format(x_i, x_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Computing gradient\n",
    "\n",
    "#### The gradient depends on the objective function: RMSE, BPR... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3227909338162291"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ij = x_i - x_j\n",
    "x_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The original BPR paper uses the logarithm of the sigmoid of x_ij, whose derivative is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7896456704188543"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = 1 / (1 + np.exp(x_ij))\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Update model\n",
    "\n",
    "#### How to update depends on the model itself, here we have just one paramether, the similarity matrix, so we perform just one update. In matrix factorization we have two.\n",
    "\n",
    "#### We need a learning rate, which influences how fast the model will change. Small ones lead to slower convergence but often higher results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "similarity_matrix[negative_item_id, negative_item_id] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usually there is no relevant change in the scores over a single iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i is 3.03, x_j is 4.34\n"
     ]
    }
   ],
   "source": [
    "x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "print(\"x_i is {:.2f}, x_j is {:.2f}\".format(x_i, x_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Write the iterative epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epochIteration():\n",
    "\n",
    "    # Get number of available interactions\n",
    "    numPositiveIteractions = int(URM_mask.nnz*0.01)\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    start_time_batch = time.time()\n",
    "\n",
    "    # Uniform user sampling without replacement\n",
    "    for num_sample in range(numPositiveIteractions):\n",
    "\n",
    "        # Sample\n",
    "        user_id, positive_item_id, negative_item_id = sampleTriplet()\n",
    "        \n",
    "        userSeenItems = URM_mask[user_id,:].indices\n",
    "        \n",
    "        # Prediction\n",
    "        x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "        x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "        \n",
    "        # Gradient\n",
    "        x_ij = x_i - x_j\n",
    "\n",
    "        gradient = 1 / (1 + np.exp(x_ij))\n",
    "        \n",
    "        # Update\n",
    "        similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "        similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "        similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "        similarity_matrix[negative_item_id, negative_item_id] = 0\n",
    "        \n",
    "\n",
    "        if(time.time() - start_time_batch >= 30 or num_sample == numPositiveIteractions-1):\n",
    "            print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                num_sample,\n",
    "                100.0* float(num_sample)/numPositiveIteractions,\n",
    "                time.time() - start_time_batch,\n",
    "                float(num_sample) / (time.time() - start_time_epoch)))\n",
    "\n",
    "\n",
    "            start_time_batch = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 905 ( 99.89% ) in 1.89 seconds. Sample per second: 478\n"
     ]
    }
   ],
   "source": [
    "epochIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "def similarityMatrixTopK(item_weights, forceSparseOutput = True, k=100, verbose = False, inplace=True):\n",
    "    \"\"\"\n",
    "    The function selects the TopK most similar elements, column-wise\n",
    "\n",
    "    :param item_weights:\n",
    "    :param forceSparseOutput:\n",
    "    :param k:\n",
    "    :param verbose:\n",
    "    :param inplace: Default True, WARNING matrix will be modified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating topK matrix\")\n",
    "\n",
    "    nitems = item_weights.shape[1]\n",
    "    k = min(k, nitems)\n",
    "\n",
    "    # for each column, keep only the top-k scored items\n",
    "    sparse_weights = not isinstance(item_weights, np.ndarray)\n",
    "\n",
    "    if not sparse_weights:\n",
    "\n",
    "        idx_sorted = np.argsort(item_weights, axis=0)  # sort data inside each column\n",
    "\n",
    "        if inplace:\n",
    "            W = item_weights\n",
    "        else:\n",
    "            W = item_weights.copy()\n",
    "\n",
    "        # index of the items that don't belong to the top-k similar items of each column\n",
    "        not_top_k = idx_sorted[:-k, :]\n",
    "        # use numpy fancy indexing to zero-out the values in sim without using a for loop\n",
    "        W[not_top_k, np.arange(nitems)] = 0.0\n",
    "\n",
    "        if forceSparseOutput:\n",
    "            W_sparse = sps.csr_matrix(W, shape=(nitems, nitems))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dense TopK matrix generated in {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "        return W\n",
    "\n",
    "    else:\n",
    "        # iterate over each column and keep only the top-k similar items\n",
    "        data, rows_indices, cols_indptr = [], [], []\n",
    "\n",
    "        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n",
    "\n",
    "        for item_idx in range(nitems):\n",
    "\n",
    "            cols_indptr.append(len(data))\n",
    "\n",
    "            start_position = item_weights.indptr[item_idx]\n",
    "            end_position = item_weights.indptr[item_idx+1]\n",
    "\n",
    "            column_data = item_weights.data[start_position:end_position]\n",
    "            column_row_index = item_weights.indices[start_position:end_position]\n",
    "\n",
    "            non_zero_data = column_data!=0\n",
    "\n",
    "            idx_sorted = np.argsort(column_data[non_zero_data])  # sort by column\n",
    "            top_k_idx = idx_sorted[-k:]\n",
    "\n",
    "            data.extend(column_data[non_zero_data][top_k_idx])\n",
    "            rows_indices.extend(column_row_index[non_zero_data][top_k_idx])\n",
    "\n",
    "\n",
    "        cols_indptr.append(len(data))\n",
    "\n",
    "        # During testing CSR is faster\n",
    "        W_sparse = sps.csc_matrix((data, rows_indices, cols_indptr), shape=(nitems, nitems), dtype=np.float32)\n",
    "        W_sparse = W_sparse.tocsr()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "        return W_sparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SLIM_BPR_Recommender(object):\n",
    "    \"\"\" SLIM_BPR recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM):\n",
    "        self.URM = URM\n",
    "        \n",
    "        self.similarity_matrix = np.zeros((n_items,n_items))\n",
    "        \n",
    "        self.URM_mask = self.URM.copy()\n",
    "        self.URM_mask.data[self.URM_mask.data < 1] = 0\n",
    "        self.URM_mask.eliminate_zeros()\n",
    "        \n",
    "        self.n_users = URM_mask.shape[0]\n",
    "        self.n_items = URM_mask.shape[1]\n",
    "\n",
    "\n",
    "        # Extract users having at least one interaction to choose from\n",
    "        self.eligibleUsers = []\n",
    "\n",
    "        for user_id in range(n_users):\n",
    "\n",
    "            start_pos = self.URM_mask.indptr[user_id]\n",
    "            end_pos = self.URM_mask.indptr[user_id+1]\n",
    "\n",
    "            if len(self.URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "                self.eligibleUsers.append(user_id)\n",
    "\n",
    "\n",
    "\n",
    "    def sampleTriplet(self):\n",
    "\n",
    "        # By randomly selecting a user in this way we could end up \n",
    "        # with a user with no interactions\n",
    "        #user_id = np.random.randint(0, n_users)\n",
    "\n",
    "        user_id = np.random.choice(self.eligibleUsers)\n",
    "\n",
    "        # Get user seen items and choose one\n",
    "        userSeenItems = URM_mask[user_id,:].indices\n",
    "        pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "        negItemSelected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        while (not negItemSelected):\n",
    "            neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "            if (neg_item_id not in userSeenItems):\n",
    "\n",
    "                negItemSelected = True\n",
    "\n",
    "        return user_id, pos_item_id, neg_item_id\n",
    "\n",
    "\n",
    "        \n",
    "    def epochIteration(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        numPositiveIteractions = int(self.URM_mask.nnz*0.01)\n",
    "\n",
    "        start_time_epoch = time.time()\n",
    "        start_time_batch = time.time()\n",
    "\n",
    "        # Uniform user sampling without replacement\n",
    "        for num_sample in range(numPositiveIteractions):\n",
    "\n",
    "            # Sample\n",
    "            user_id, positive_item_id, negative_item_id = self.sampleTriplet()\n",
    "\n",
    "            userSeenItems = self.URM_mask[user_id,:].indices\n",
    "\n",
    "            # Prediction\n",
    "            x_i = self.similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "            x_j = self.similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "            # Gradient\n",
    "            x_ij = x_i - x_j\n",
    "\n",
    "            gradient = 1 / (1 + np.exp(x_ij))\n",
    "\n",
    "            # Update\n",
    "            self.similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "            self.similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "            self.similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "            self.similarity_matrix[negative_item_id, negative_item_id] = 0\n",
    "\n",
    "\n",
    "            if(time.time() - start_time_batch >= 30 or num_sample == numPositiveIteractions-1):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                    num_sample,\n",
    "                    100.0* float(num_sample)/numPositiveIteractions,\n",
    "                    time.time() - start_time_batch,\n",
    "                    float(num_sample) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                start_time_batch = time.time()\n",
    "\n",
    "                \n",
    "    def fit(self, learning_rate = 0.01, epochs = 10):\n",
    " \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        for numEpoch in range(self.epochs):\n",
    "            self.epochIteration()\n",
    "            \n",
    "        self.similarity_matrix = self.similarity_matrix.T\n",
    "        \n",
    "        self.similarity_matrix = similarityMatrixTopK(self.similarity_matrix, k=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.similarity_matrix).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 905 ( 99.89% ) in 0.91 seconds. Sample per second: 993\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIM_BPR_Recommender(URM_train)\n",
    "recommender.fit(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated user 0 of 7947\n",
      "Recommender performance is: Precision = 0.0040, Recall = 0.0065, MAP = 0.0049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.003955500618046964,\n",
       " 'recall': 0.0064557170134016606,\n",
       " 'MAP': 0.004903614103243273}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Notebooks_utils.evaluation_function import evaluate_algorithm\n",
    "\n",
    "evaluate_algorithm(URM_test, recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: URM Detected 77 (0.97 %) cold users.\n",
      "SLIM_BPR_Recommender: URM Detected 2453 (9.44 %) cold items.\n",
      "Unable to read memory status: list index out of range\n",
      "SLIM_BPR_Recommender: Automatic selection of fastest train mode. Unable to get current RAM status, you may be using a non-Linux operating system. Using dense matrix.\n",
      "Processed 7947 ( 100.00% ) in 0.50 seconds. BPR loss is 1.97E-08. Sample per second: 15885\n",
      "SLIM_BPR_Recommender: Epoch 1 of 10. Elapsed time 0.02 sec\n",
      "Processed 7947 ( 100.00% ) in 0.52 seconds. BPR loss is 6.44E-08. Sample per second: 15307\n",
      "SLIM_BPR_Recommender: Epoch 2 of 10. Elapsed time 0.03 sec\n",
      "Processed 7947 ( 100.00% ) in 0.54 seconds. BPR loss is 1.46E-07. Sample per second: 14699\n",
      "SLIM_BPR_Recommender: Epoch 3 of 10. Elapsed time 0.06 sec\n",
      "Processed 7947 ( 100.00% ) in 0.56 seconds. BPR loss is 2.57E-07. Sample per second: 14176\n",
      "SLIM_BPR_Recommender: Epoch 4 of 10. Elapsed time 0.08 sec\n",
      "Processed 7947 ( 100.00% ) in 0.58 seconds. BPR loss is 4.23E-07. Sample per second: 13691\n",
      "SLIM_BPR_Recommender: Epoch 5 of 10. Elapsed time 0.10 sec\n",
      "Processed 7947 ( 100.00% ) in 0.60 seconds. BPR loss is 8.47E-07. Sample per second: 13233\n",
      "SLIM_BPR_Recommender: Epoch 6 of 10. Elapsed time 0.12 sec\n",
      "Processed 7947 ( 100.00% ) in 0.62 seconds. BPR loss is 7.25E-07. Sample per second: 12824\n",
      "SLIM_BPR_Recommender: Epoch 7 of 10. Elapsed time 0.14 sec\n",
      "Processed 7947 ( 100.00% ) in 0.64 seconds. BPR loss is 9.53E-07. Sample per second: 12404\n",
      "SLIM_BPR_Recommender: Epoch 8 of 10. Elapsed time 0.16 sec\n",
      "Processed 7947 ( 100.00% ) in 0.66 seconds. BPR loss is 1.16E-06. Sample per second: 12045\n",
      "SLIM_BPR_Recommender: Epoch 9 of 10. Elapsed time 0.18 sec\n",
      "Processed 7947 ( 100.00% ) in 0.68 seconds. BPR loss is 1.45E-06. Sample per second: 11654\n",
      "SLIM_BPR_Recommender: Epoch 10 of 10. Elapsed time 0.20 sec\n",
      "SLIM_BPR_Recommender: Terminating at epoch 10. Elapsed time 13.36 sec\n",
      "Deallocating Cython objects\n"
     ]
    }
   ],
   "source": [
    "from SLIM_BPR.Cython.SLIM_BPR_Cython import SLIM_BPR_Cython\n",
    "\n",
    "\n",
    "\n",
    "recommender = SLIM_BPR_Cython(URM_train, recompile_cython=False)\n",
    "\n",
    "recommender.fit(epochs=10, batch_size=1, sgd_mode='sgd', learning_rate=1e-4, positive_threshold_BPR=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Processed 5663 ( 100.00% ) in 3.21 sec. Users per second: 1764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({5: {'ROC_AUC': 0.09623874271587492,\n",
       "   'PRECISION': 0.03775384072046581,\n",
       "   'PRECISION_RECALL_MIN_DEN': 0.07308552592854156,\n",
       "   'RECALL': 0.06499789142705964,\n",
       "   'MAP': 0.043094698530421534,\n",
       "   'MRR': 0.09212431573370977,\n",
       "   'NDCG': 0.05607747518607622,\n",
       "   'F1': 0.04776406175966225,\n",
       "   'HIT_RATE': 0.18876920360233093,\n",
       "   'ARHR': 0.10105067985166864,\n",
       "   'NOVELTY': 0.002399682062707185,\n",
       "   'AVERAGE_POPULARITY': 0.12868208471976456,\n",
       "   'DIVERSITY_MEAN_INTER_LIST': 0.9861297372815402,\n",
       "   'DIVERSITY_HERFINDAHL': 0.9971911203421536,\n",
       "   'COVERAGE_ITEM': 0.2534744947064485,\n",
       "   'COVERAGE_ITEM_CORRECT': 0.020134744947064485,\n",
       "   'COVERAGE_USER': 0.712595948156537,\n",
       "   'COVERAGE_USER_CORRECT': 0.11337611677362527,\n",
       "   'DIVERSITY_GINI': 0.08680949790461166,\n",
       "   'SHANNON_ENTROPY': 10.734623959280022}},\n",
       " 'CUTOFF: 5 - ROC_AUC: 0.0962387, PRECISION: 0.0377538, PRECISION_RECALL_MIN_DEN: 0.0730855, RECALL: 0.0649979, MAP: 0.0430947, MRR: 0.0921243, NDCG: 0.0560775, F1: 0.0477641, HIT_RATE: 0.1887692, ARHR: 0.1010507, NOVELTY: 0.0023997, AVERAGE_POPULARITY: 0.1286821, DIVERSITY_MEAN_INTER_LIST: 0.9861297, DIVERSITY_HERFINDAHL: 0.9971911, COVERAGE_ITEM: 0.2534745, COVERAGE_ITEM_CORRECT: 0.0201347, COVERAGE_USER: 0.7125959, COVERAGE_USER_CORRECT: 0.1133761, DIVERSITY_GINI: 0.0868095, SHANNON_ENTROPY: 10.7346240, \\n')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_test, cutoff_list=[5])\n",
    "\n",
    "\n",
    "evaluator_validation.evaluateRecommender(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: URM Detected 1079 (4.15 %) cold items.\n",
      "Deallocating Cython objects\n",
      "Unable to read memory status: list index out of range\n",
      "SLIM_BPR_Recommender: Automatic selection of fastest train mode. Unable to get current RAM status, you may be using a non-Linux operating system. Using dense matrix.\n",
      "Processed 7947 ( 100.00% ) in 0.45 seconds. BPR loss is 3.11E-08. Sample per second: 17607\n",
      "SLIM_BPR_Recommender: Epoch 1 of 10. Elapsed time 0.02 sec\n",
      "Processed 7947 ( 100.00% ) in 0.47 seconds. BPR loss is 1.28E-07. Sample per second: 16733\n",
      "SLIM_BPR_Recommender: Epoch 2 of 10. Elapsed time 0.04 sec\n",
      "Processed 7947 ( 100.00% ) in 0.50 seconds. BPR loss is 2.12E-07. Sample per second: 16004\n",
      "SLIM_BPR_Recommender: Epoch 3 of 10. Elapsed time 0.06 sec\n",
      "Processed 7947 ( 100.00% ) in 0.52 seconds. BPR loss is 4.46E-07. Sample per second: 15282\n",
      "SLIM_BPR_Recommender: Epoch 4 of 10. Elapsed time 0.09 sec\n",
      "Processed 7947 ( 100.00% ) in 0.54 seconds. BPR loss is 6.84E-07. Sample per second: 14634\n",
      "SLIM_BPR_Recommender: Epoch 5 of 10. Elapsed time 0.11 sec\n",
      "Processed 7947 ( 100.00% ) in 0.57 seconds. BPR loss is 9.36E-07. Sample per second: 14038\n",
      "SLIM_BPR_Recommender: Epoch 6 of 10. Elapsed time 0.13 sec\n",
      "Processed 7947 ( 100.00% ) in 0.59 seconds. BPR loss is 1.33E-06. Sample per second: 13484\n",
      "SLIM_BPR_Recommender: Epoch 7 of 10. Elapsed time 0.16 sec\n",
      "Processed 7947 ( 100.00% ) in 0.61 seconds. BPR loss is 1.58E-06. Sample per second: 12973\n",
      "SLIM_BPR_Recommender: Epoch 8 of 10. Elapsed time 0.18 sec\n",
      "Processed 7947 ( 100.00% ) in 0.64 seconds. BPR loss is 2.05E-06. Sample per second: 12467\n",
      "SLIM_BPR_Recommender: Epoch 9 of 10. Elapsed time 0.21 sec\n",
      "Processed 7947 ( 100.00% ) in 0.66 seconds. BPR loss is 2.45E-06. Sample per second: 12043\n",
      "SLIM_BPR_Recommender: Epoch 10 of 10. Elapsed time 0.23 sec\n",
      "SLIM_BPR_Recommender: Terminating at epoch 10. Elapsed time 13.37 sec\n",
      "Deallocating Cython objects\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIM_BPR_Cython(URM_all, recompile_cython=False)\n",
    "\n",
    "recommender.fit(epochs=10, batch_size=1, sgd_mode='sgd', learning_rate=1e-4, positive_threshold_BPR=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoms = recommender.recommend(userTestList, cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomList = []\n",
    "for i in range(len(recoms)):\n",
    "    recomList.append(' '.join(str(e) for e in recoms[i]))\n",
    "# print(recoms)\n",
    "\n",
    "res = {\"user_id\": userTestList, \"item_list\": recomList}\n",
    "result = pd.DataFrame(res, columns= ['user_id', 'item_list'])\n",
    "\n",
    "result.to_csv ('outputs/slim_brp1.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLIM ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import time, sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SLIMElasticNetRecommender(object):\n",
    "    \"\"\"\n",
    "    Train a Sparse Linear Methods (SLIM) item similarity model.\n",
    "    NOTE: ElasticNet solver is parallel, a single intance of SLIM_ElasticNet will\n",
    "          make use of half the cores available\n",
    "\n",
    "    See:\n",
    "        Efficient Top-N Recommendation by Linear Regression,\n",
    "        M. Levy and K. Jack, LSRS workshop at RecSys 2013.\n",
    "        https://www.slideshare.net/MarkLevy/efficient-slides\n",
    "\n",
    "        SLIM: Sparse linear methods for top-n recommender systems,\n",
    "        X. Ning and G. Karypis, ICDM 2011.\n",
    "        http://glaros.dtc.umn.edu/gkhome/fetch/papers/SLIM2011icdm.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"SLIMElasticNetRecommender\"\n",
    "\n",
    "    def __init__(self, URM_train):\n",
    "\n",
    "        super(SLIMElasticNetRecommender, self).__init__()\n",
    "\n",
    "        self.URM_train = URM_train\n",
    "\n",
    "\n",
    "    def fit(self, l1_penalty=0.1, l2_penalty=0.1, positive_only=True, topK = 100):\n",
    "\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.positive_only = positive_only\n",
    "        self.topK = topK\n",
    "\n",
    "        if self.l1_penalty + self.l2_penalty != 0:\n",
    "            self.l1_ratio = self.l1_penalty / (self.l1_penalty + self.l2_penalty)\n",
    "        else:\n",
    "            print(\"SLIM_ElasticNet: l1_penalty+l2_penalty cannot be equal to zero, setting the ratio l1/(l1+l2) to 1.0\")\n",
    "            self.l1_ratio = 1.0\n",
    "\n",
    "\n",
    "\n",
    "        # initialize the ElasticNet model\n",
    "        self.model = ElasticNet(alpha=1.0,\n",
    "                                l1_ratio=self.l1_ratio,\n",
    "                                positive=self.positive_only,\n",
    "                                fit_intercept=False,\n",
    "                                copy_X=False,\n",
    "                                precompute=True,\n",
    "                                selection='random',\n",
    "                                max_iter=100,\n",
    "                                tol=1e-4)\n",
    "\n",
    "\n",
    "        URM_train = sps.csc_matrix(self.URM_train)\n",
    "\n",
    "        n_items = URM_train.shape[1]\n",
    "\n",
    "\n",
    "        # Use array as it reduces memory requirements compared to lists\n",
    "        dataBlock = 10000000\n",
    "\n",
    "        rows = np.zeros(dataBlock, dtype=np.int32)\n",
    "        cols = np.zeros(dataBlock, dtype=np.int32)\n",
    "        values = np.zeros(dataBlock, dtype=np.float32)\n",
    "\n",
    "        numCells = 0\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_printBatch = start_time\n",
    "\n",
    "        # fit each item's factors sequentially (not in parallel)\n",
    "        for currentItem in range(n_items):\n",
    "\n",
    "            # get the target column\n",
    "            y = URM_train[:, currentItem].toarray()\n",
    "\n",
    "            # set the j-th column of X to zero\n",
    "            start_pos = URM_train.indptr[currentItem]\n",
    "            end_pos = URM_train.indptr[currentItem + 1]\n",
    "\n",
    "            current_item_data_backup = URM_train.data[start_pos: end_pos].copy()\n",
    "            URM_train.data[start_pos: end_pos] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "            # fit one ElasticNet model per column\n",
    "            self.model.fit(URM_train, y)\n",
    "\n",
    "            # self.model.coef_ contains the coefficient of the ElasticNet model\n",
    "            # let's keep only the non-zero values\n",
    "\n",
    "            # Select topK values\n",
    "            # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "            # - Partition the data to extract the set of relevant items\n",
    "            # - Sort only the relevant items\n",
    "            # - Get the original item index\n",
    "\n",
    "            nonzero_model_coef_index = self.model.sparse_coef_.indices\n",
    "            nonzero_model_coef_value = self.model.sparse_coef_.data\n",
    "\n",
    "\n",
    "            local_topK = min(len(nonzero_model_coef_value)-1, self.topK)\n",
    "\n",
    "            relevant_items_partition = (-nonzero_model_coef_value).argpartition(local_topK)[0:local_topK]\n",
    "            relevant_items_partition_sorting = np.argsort(-nonzero_model_coef_value[relevant_items_partition])\n",
    "            ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "\n",
    "            for index in range(len(ranking)):\n",
    "\n",
    "                if numCells == len(rows):\n",
    "                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n",
    "                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n",
    "                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n",
    "\n",
    "\n",
    "                rows[numCells] = nonzero_model_coef_index[ranking[index]]\n",
    "                cols[numCells] = currentItem\n",
    "                values[numCells] = nonzero_model_coef_value[ranking[index]]\n",
    "\n",
    "                numCells += 1\n",
    "\n",
    "\n",
    "            # finally, replace the original values of the j-th column\n",
    "            URM_train.data[start_pos:end_pos] = current_item_data_backup\n",
    "\n",
    "\n",
    "            if time.time() - start_time_printBatch > 300 or currentItem == n_items-1:\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} minutes. Items per second: {:.0f}\".format(\n",
    "                                  currentItem+1,\n",
    "                                  100.0* float(currentItem+1)/n_items,\n",
    "                                  (time.time()-start_time)/60,\n",
    "                                  float(currentItem)/(time.time()-start_time)))\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_printBatch = time.time()\n",
    "\n",
    "\n",
    "        # generate the sparse weight matrix\n",
    "        self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])),\n",
    "                                       shape=(n_items, n_items), dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM_train[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM_train.indptr[user_id]\n",
    "        end_pos = self.URM_train.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM_train.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deallocating Cython objects\n",
      "Processed 16567 ( 63.78% ) in 5.00 minutes. Items per second: 55\n",
      "Processed 25975 ( 100.00% ) in 7.86 minutes. Items per second: 55\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIMElasticNetRecommender(URM_train)\n",
    "\n",
    "recommender.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated user 0 of 7947\n",
      "Recommender performance is: Precision = 0.0002, Recall = 0.0001, MAP = 0.0001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.0002119018188239449,\n",
       " 'recall': 5.517695072137684e-05,\n",
       " 'MAP': 8.358349520277827e-05}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_algorithm(URM_test, recommender)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
