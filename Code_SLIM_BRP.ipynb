{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy.sparse as sps\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"./input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RM_train=pd.read_csv('./input/data_train.csv')\n",
    "R_test=pd.read_csv('./input/data_target_users_test.csv')\n",
    "URM=pd.read_csv('./input/data_train.csv')\n",
    "ICM = pd.read_csv('./input/data_ICM_title_abstract.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## URM all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_tuples = [tuple(x) for x in URM.to_numpy()]\n",
    "\n",
    "userList, itemList, ratingList = zip(*URM_tuples)\n",
    "\n",
    "userList = list(userList)\n",
    "userList=np.array(userList,dtype=np.int64)\n",
    "itemList = list(itemList)\n",
    "itemList=np.array(itemList,dtype=np.int64)\n",
    "\n",
    "ratingList = list(ratingList)                        #not needed\n",
    "ratingList=np.array(ratingList,dtype=np.int64)       #not needed\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all = URM_all.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items\t 24896, Number of users\t 7947\n",
      "Number of Intraction \t 113268\n",
      "Max ID items\t 25974, Max Id users\t 7946\n",
      "\n",
      "Average interactions per user 14.25\n",
      "Average interactions per item 4.55\n",
      "\n",
      "Sparsity 99.94 %\n"
     ]
    }
   ],
   "source": [
    "userList_unique = list(set(userList))\n",
    "itemList_unique = list(set(itemList))\n",
    "\n",
    "numUsers = len(userList_unique)\n",
    "numItems = len(itemList_unique)\n",
    "\n",
    "numberInteractions= len(URM_tuples)\n",
    "print (\"Number of items\\t {}, Number of users\\t {}\".format(numItems, numUsers))\n",
    "print(\"Number of Intraction \\t {}\" .format(numberInteractions))\n",
    "print (\"Max ID items\\t {}, Max Id users\\t {}\\n\".format(max(itemList_unique), max(userList_unique)))\n",
    "print (\"Average interactions per user {:.2f}\".format(numberInteractions/numUsers))\n",
    "print (\"Average interactions per item {:.2f}\\n\".format(numberInteractions/numItems))\n",
    "\n",
    "print (\"Sparsity {:.2f} %\".format((1-float(numberInteractions)/(numItems*numUsers))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ICM all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25975x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 490691 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ICM_tuples = [tuple(x) for x in ICM.to_numpy()]\n",
    "itemList_icm, featureList_icm, scoreList_icm = zip(*ICM_tuples)\n",
    "\n",
    "itemList_icm = list(itemList_icm)\n",
    "itemList_icm = np.array(itemList_icm,dtype=np.int64)\n",
    "\n",
    "featureList_icm = list(featureList_icm)\n",
    "featureList_icm = np.array(featureList_icm,dtype=np.int64)\n",
    "\n",
    "scoreList_icm = list(scoreList_icm)\n",
    "scoreList_icm = np.array(scoreList_icm,dtype=np.float64)\n",
    "\n",
    "ICM_all = sps.coo_matrix((scoreList_icm, (itemList_icm, featureList_icm)))\n",
    "\n",
    "ICM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userTestList = [x for x in R_test.to_numpy()]\n",
    "userTestList = zip(*userTestList)\n",
    "userTestList = [list(a) for a in userTestList][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 77 (0.97 %) of 7947 users have no train items\n",
      "Warning: 2340 (29.45 %) of 7947 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.80)\n",
    "# URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train, train_percentage = 0.80)\n",
    "\n",
    "# evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement SLIM BPR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Sampling\n",
    "\n",
    "#### Create a mask of positive interactions. How to build it depends on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7947x25975 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 90614 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_mask = URM_train.copy()\n",
    "URM_mask.data[URM_mask.data < 1] = 0\n",
    "\n",
    "URM_mask.eliminate_zeros()\n",
    "URM_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = URM_mask.shape[0]\n",
    "n_items = URM_mask.shape[1]\n",
    "\n",
    "\n",
    "# Extract users having at least one interaction to choose from\n",
    "eligibleUsers = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "    start_pos = URM_mask.indptr[user_id]\n",
    "    end_pos = URM_mask.indptr[user_id+1]\n",
    "    if len(URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "        eligibleUsers.append(user_id)\n",
    "                \n",
    "\n",
    "def sampleTriplet():\n",
    "    \n",
    "    # By randomly selecting a user in this way we could end up \n",
    "    # with a user with no interactions\n",
    "    #user_id = np.random.randint(0, n_users)\n",
    "    \n",
    "    user_id = np.random.choice(eligibleUsers)\n",
    "    \n",
    "    # Get user seen items and choose one\n",
    "    userSeenItems = URM_mask[user_id,:].indices\n",
    "    pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "    negItemSelected = False\n",
    "\n",
    "    # It's faster to just try again then to build a mapping of the non-seen items\n",
    "    while (not negItemSelected):\n",
    "        neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "        if (neg_item_id not in userSeenItems):\n",
    "            \n",
    "            negItemSelected = True\n",
    "\n",
    "    return user_id, pos_item_id, neg_item_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4033, 14140, 11281)\n",
      "(4697, 20222, 9922)\n",
      "(1712, 4804, 22414)\n",
      "(6976, 15269, 13412)\n",
      "(1259, 12914, 11529)\n",
      "(3711, 17442, 10000)\n",
      "(4248, 13552, 8017)\n",
      "(4776, 23943, 5448)\n",
      "(2934, 10689, 5761)\n",
      "(7323, 24721, 18953)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampleTriplet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Computing prediction\n",
    "\n",
    "#### The prediction depends on the model: SLIM, Matrix Factorization... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to initialize our model. In case of SLIM it works best to initialize S as zero, in case of MF you cannot because of how the gradient is computed and you have to initialize at random. Here we initialize SLIM at random just so that we have some numbers to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.random.random((n_items,n_items))\n",
    "similarity_matrix[np.arange(n_items),np.arange(n_items)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, positive_item_id, negative_item_id = sampleTriplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13309"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22376"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1331,  3907,  6952,  8360,  9018, 13309, 14562, 18417, 18495,\n",
       "       19618, 21936, 22217, 22848, 24672], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userSeenItems = URM_mask[user_id,:].indices\n",
    "userSeenItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i is 6.08, x_j is 6.03\n"
     ]
    }
   ],
   "source": [
    "x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "print(\"x_i is {:.2f}, x_j is {:.2f}\".format(x_i, x_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Computing gradient\n",
    "\n",
    "#### The gradient depends on the objective function: RMSE, BPR... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052418597579121595"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ij = x_i - x_j\n",
    "x_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The original BPR paper uses the logarithm of the sigmoid of x_ij, whose derivative is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4868983504282882"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = 1 / (1 + np.exp(x_ij))\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Update model\n",
    "\n",
    "#### How to update depends on the model itself, here we have just one paramether, the similarity matrix, so we perform just one update. In matrix factorization we have two.\n",
    "\n",
    "#### We need a learning rate, which influences how fast the model will change. Small ones lead to slower convergence but often higher results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "similarity_matrix[negative_item_id, negative_item_id] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usually there is no relevant change in the scores over a single iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i is 6.08, x_j is 6.02\n"
     ]
    }
   ],
   "source": [
    "x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "print(\"x_i is {:.2f}, x_j is {:.2f}\".format(x_i, x_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Write the iterative epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epochIteration():\n",
    "\n",
    "    # Get number of available interactions\n",
    "    numPositiveIteractions = int(URM_mask.nnz*0.01)\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    start_time_batch = time.time()\n",
    "\n",
    "    # Uniform user sampling without replacement\n",
    "    for num_sample in range(numPositiveIteractions):\n",
    "\n",
    "        # Sample\n",
    "        user_id, positive_item_id, negative_item_id = sampleTriplet()\n",
    "        \n",
    "        userSeenItems = URM_mask[user_id,:].indices\n",
    "        \n",
    "        # Prediction\n",
    "        x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "        x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "        \n",
    "        # Gradient\n",
    "        x_ij = x_i - x_j\n",
    "\n",
    "        gradient = 1 / (1 + np.exp(x_ij))\n",
    "        \n",
    "        # Update\n",
    "        similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "        similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "        similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "        similarity_matrix[negative_item_id, negative_item_id] = 0\n",
    "        \n",
    "\n",
    "        if(time.time() - start_time_batch >= 30 or num_sample == numPositiveIteractions-1):\n",
    "            print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                num_sample,\n",
    "                100.0* float(num_sample)/numPositiveIteractions,\n",
    "                time.time() - start_time_batch,\n",
    "                float(num_sample) / (time.time() - start_time_epoch)))\n",
    "\n",
    "\n",
    "            start_time_batch = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 905 ( 99.89% ) in 1.71 seconds. Sample per second: 530\n"
     ]
    }
   ],
   "source": [
    "epochIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "def similarityMatrixTopK(item_weights, forceSparseOutput = True, k=100, verbose = False, inplace=True):\n",
    "    \"\"\"\n",
    "    The function selects the TopK most similar elements, column-wise\n",
    "\n",
    "    :param item_weights:\n",
    "    :param forceSparseOutput:\n",
    "    :param k:\n",
    "    :param verbose:\n",
    "    :param inplace: Default True, WARNING matrix will be modified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating topK matrix\")\n",
    "\n",
    "    nitems = item_weights.shape[1]\n",
    "    k = min(k, nitems)\n",
    "\n",
    "    # for each column, keep only the top-k scored items\n",
    "    sparse_weights = not isinstance(item_weights, np.ndarray)\n",
    "\n",
    "    if not sparse_weights:\n",
    "\n",
    "        idx_sorted = np.argsort(item_weights, axis=0)  # sort data inside each column\n",
    "\n",
    "        if inplace:\n",
    "            W = item_weights\n",
    "        else:\n",
    "            W = item_weights.copy()\n",
    "\n",
    "        # index of the items that don't belong to the top-k similar items of each column\n",
    "        not_top_k = idx_sorted[:-k, :]\n",
    "        # use numpy fancy indexing to zero-out the values in sim without using a for loop\n",
    "        W[not_top_k, np.arange(nitems)] = 0.0\n",
    "\n",
    "        if forceSparseOutput:\n",
    "            W_sparse = sps.csr_matrix(W, shape=(nitems, nitems))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dense TopK matrix generated in {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "        return W\n",
    "\n",
    "    else:\n",
    "        # iterate over each column and keep only the top-k similar items\n",
    "        data, rows_indices, cols_indptr = [], [], []\n",
    "\n",
    "        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n",
    "\n",
    "        for item_idx in range(nitems):\n",
    "\n",
    "            cols_indptr.append(len(data))\n",
    "\n",
    "            start_position = item_weights.indptr[item_idx]\n",
    "            end_position = item_weights.indptr[item_idx+1]\n",
    "\n",
    "            column_data = item_weights.data[start_position:end_position]\n",
    "            column_row_index = item_weights.indices[start_position:end_position]\n",
    "\n",
    "            non_zero_data = column_data!=0\n",
    "\n",
    "            idx_sorted = np.argsort(column_data[non_zero_data])  # sort by column\n",
    "            top_k_idx = idx_sorted[-k:]\n",
    "\n",
    "            data.extend(column_data[non_zero_data][top_k_idx])\n",
    "            rows_indices.extend(column_row_index[non_zero_data][top_k_idx])\n",
    "\n",
    "\n",
    "        cols_indptr.append(len(data))\n",
    "\n",
    "        # During testing CSR is faster\n",
    "        W_sparse = sps.csc_matrix((data, rows_indices, cols_indptr), shape=(nitems, nitems), dtype=np.float32)\n",
    "        W_sparse = W_sparse.tocsr()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "        return W_sparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SLIM_BPR_Recommender(object):\n",
    "    \"\"\" SLIM_BPR recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM):\n",
    "        self.URM = URM\n",
    "        \n",
    "        self.similarity_matrix = np.zeros((n_items,n_items))\n",
    "        \n",
    "        self.URM_mask = self.URM.copy()\n",
    "        self.URM_mask.data[self.URM_mask.data < 1] = 0\n",
    "        self.URM_mask.eliminate_zeros()\n",
    "        \n",
    "        self.n_users = URM_mask.shape[0]\n",
    "        self.n_items = URM_mask.shape[1]\n",
    "\n",
    "\n",
    "        # Extract users having at least one interaction to choose from\n",
    "        self.eligibleUsers = []\n",
    "\n",
    "        for user_id in range(n_users):\n",
    "\n",
    "            start_pos = self.URM_mask.indptr[user_id]\n",
    "            end_pos = self.URM_mask.indptr[user_id+1]\n",
    "\n",
    "            if len(self.URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "                self.eligibleUsers.append(user_id)\n",
    "\n",
    "\n",
    "\n",
    "    def sampleTriplet(self):\n",
    "\n",
    "        # By randomly selecting a user in this way we could end up \n",
    "        # with a user with no interactions\n",
    "        #user_id = np.random.randint(0, n_users)\n",
    "\n",
    "        user_id = np.random.choice(self.eligibleUsers)\n",
    "\n",
    "        # Get user seen items and choose one\n",
    "        userSeenItems = URM_mask[user_id,:].indices\n",
    "        pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "        negItemSelected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        while (not negItemSelected):\n",
    "            neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "            if (neg_item_id not in userSeenItems):\n",
    "\n",
    "                negItemSelected = True\n",
    "\n",
    "        return user_id, pos_item_id, neg_item_id\n",
    "\n",
    "\n",
    "        \n",
    "    def epochIteration(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        numPositiveIteractions = int(self.URM_mask.nnz*0.01)\n",
    "\n",
    "        start_time_epoch = time.time()\n",
    "        start_time_batch = time.time()\n",
    "\n",
    "        # Uniform user sampling without replacement\n",
    "        for num_sample in range(numPositiveIteractions):\n",
    "\n",
    "            # Sample\n",
    "            user_id, positive_item_id, negative_item_id = self.sampleTriplet()\n",
    "\n",
    "            userSeenItems = self.URM_mask[user_id,:].indices\n",
    "\n",
    "            # Prediction\n",
    "            x_i = self.similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "            x_j = self.similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "            # Gradient\n",
    "            x_ij = x_i - x_j\n",
    "\n",
    "            gradient = 1 / (1 + np.exp(x_ij))\n",
    "\n",
    "            # Update\n",
    "            self.similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "            self.similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "            self.similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "            self.similarity_matrix[negative_item_id, negative_item_id] = 0\n",
    "\n",
    "\n",
    "            if(time.time() - start_time_batch >= 30 or num_sample == numPositiveIteractions-1):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                    num_sample,\n",
    "                    100.0* float(num_sample)/numPositiveIteractions,\n",
    "                    time.time() - start_time_batch,\n",
    "                    float(num_sample) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                start_time_batch = time.time()\n",
    "\n",
    "                \n",
    "    def fit(self, learning_rate = 0.01, epochs = 10):\n",
    " \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        for numEpoch in range(self.epochs):\n",
    "            self.epochIteration()\n",
    "            \n",
    "        self.similarity_matrix = self.similarity_matrix.T\n",
    "        \n",
    "        self.similarity_matrix = similarityMatrixTopK(self.similarity_matrix, k=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.similarity_matrix).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 905 ( 99.89% ) in 0.79 seconds. Sample per second: 1151\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIM_BPR_Recommender(URM_train)\n",
    "recommender.fit(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated user 0 of 7947\n",
      "Recommender performance is: Precision = 0.0048, Recall = 0.0077, MAP = 0.0058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.004815409309791321,\n",
       " 'recall': 0.007749077250882628,\n",
       " 'MAP': 0.005782206765352834}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Notebooks_utils.evaluation_function import evaluate_algorithm\n",
    "\n",
    "evaluate_algorithm(URM_test, recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: URM Detected 77 (0.97 %) cold users.\n",
      "SLIM_BPR_Recommender: URM Detected 2360 (9.09 %) cold items.\n",
      "Deallocating Cython objects\n",
      "Unable to read memory status: list index out of range\n",
      "SLIM_BPR_Recommender: Automatic selection of fastest train mode. Unable to get current RAM status, you may be using a non-Linux operating system. Using dense matrix.\n",
      "Processed 7947 ( 100.00% ) in 0.26 seconds. BPR loss is 1.84E-08. Sample per second: 31100\n",
      "SLIM_BPR_Recommender: Epoch 1 of 50. Elapsed time 0.18 sec\n",
      "Processed 7947 ( 100.00% ) in 0.50 seconds. BPR loss is 7.39E-08. Sample per second: 15881\n",
      "SLIM_BPR_Recommender: Epoch 2 of 50. Elapsed time 0.42 sec\n",
      "Processed 7947 ( 100.00% ) in 0.91 seconds. BPR loss is 1.69E-07. Sample per second: 8705\n",
      "SLIM_BPR_Recommender: Epoch 3 of 50. Elapsed time 0.83 sec\n",
      "Processed 7947 ( 100.00% ) in 1.09 seconds. BPR loss is 2.66E-07. Sample per second: 7271\n",
      "SLIM_BPR_Recommender: Epoch 4 of 50. Elapsed time 1.01 sec\n",
      "Processed 7947 ( 100.00% ) in 0.31 seconds. BPR loss is 4.18E-07. Sample per second: 25311\n",
      "SLIM_BPR_Recommender: Epoch 5 of 50. Elapsed time 1.23 sec\n",
      "Processed 7947 ( 100.00% ) in 0.40 seconds. BPR loss is 5.46E-07. Sample per second: 20024\n",
      "SLIM_BPR_Recommender: Epoch 6 of 50. Elapsed time 1.31 sec\n",
      "Processed 7947 ( 100.00% ) in 0.47 seconds. BPR loss is 7.04E-07. Sample per second: 16784\n",
      "SLIM_BPR_Recommender: Epoch 7 of 50. Elapsed time 1.39 sec\n",
      "Processed 7947 ( 100.00% ) in 0.54 seconds. BPR loss is 9.34E-07. Sample per second: 14749\n",
      "SLIM_BPR_Recommender: Epoch 8 of 50. Elapsed time 1.45 sec\n",
      "Processed 7947 ( 100.00% ) in 0.61 seconds. BPR loss is 1.26E-06. Sample per second: 13126\n",
      "SLIM_BPR_Recommender: Epoch 9 of 50. Elapsed time 1.52 sec\n",
      "Processed 7947 ( 100.00% ) in 0.66 seconds. BPR loss is 1.45E-06. Sample per second: 11999\n",
      "SLIM_BPR_Recommender: Epoch 10 of 50. Elapsed time 1.58 sec\n",
      "Processed 7947 ( 100.00% ) in 0.72 seconds. BPR loss is 1.85E-06. Sample per second: 11012\n",
      "SLIM_BPR_Recommender: Epoch 11 of 50. Elapsed time 1.64 sec\n",
      "Processed 7947 ( 100.00% ) in 0.77 seconds. BPR loss is 2.04E-06. Sample per second: 10281\n",
      "SLIM_BPR_Recommender: Epoch 12 of 50. Elapsed time 1.69 sec\n",
      "Processed 7947 ( 100.00% ) in 0.82 seconds. BPR loss is 2.48E-06. Sample per second: 9662\n",
      "SLIM_BPR_Recommender: Epoch 13 of 50. Elapsed time 1.74 sec\n",
      "Processed 7947 ( 100.00% ) in 0.87 seconds. BPR loss is 3.47E-06. Sample per second: 9111\n",
      "SLIM_BPR_Recommender: Epoch 14 of 50. Elapsed time 1.79 sec\n",
      "Processed 7947 ( 100.00% ) in 0.92 seconds. BPR loss is 3.16E-06. Sample per second: 8626\n",
      "SLIM_BPR_Recommender: Epoch 15 of 50. Elapsed time 1.84 sec\n",
      "Processed 7947 ( 100.00% ) in 0.97 seconds. BPR loss is 3.74E-06. Sample per second: 8234\n",
      "SLIM_BPR_Recommender: Epoch 16 of 50. Elapsed time 1.88 sec\n",
      "Processed 7947 ( 100.00% ) in 1.01 seconds. BPR loss is 4.09E-06. Sample per second: 7879\n",
      "SLIM_BPR_Recommender: Epoch 17 of 50. Elapsed time 1.92 sec\n",
      "Processed 7947 ( 100.00% ) in 0.05 seconds. BPR loss is 4.54E-06. Sample per second: 170752\n",
      "SLIM_BPR_Recommender: Epoch 18 of 50. Elapsed time 1.96 sec\n",
      "Processed 7947 ( 100.00% ) in 0.09 seconds. BPR loss is 5.23E-06. Sample per second: 88693\n",
      "SLIM_BPR_Recommender: Epoch 19 of 50. Elapsed time 2.01 sec\n",
      "Processed 7947 ( 100.00% ) in 0.13 seconds. BPR loss is 5.60E-06. Sample per second: 60390\n",
      "SLIM_BPR_Recommender: Epoch 20 of 50. Elapsed time 2.05 sec\n",
      "Processed 7947 ( 100.00% ) in 0.17 seconds. BPR loss is 6.38E-06. Sample per second: 46436\n",
      "SLIM_BPR_Recommender: Epoch 21 of 50. Elapsed time 2.09 sec\n",
      "Processed 7947 ( 100.00% ) in 0.21 seconds. BPR loss is 6.80E-06. Sample per second: 37537\n",
      "SLIM_BPR_Recommender: Epoch 22 of 50. Elapsed time 2.13 sec\n",
      "Processed 7947 ( 100.00% ) in 0.26 seconds. BPR loss is 7.31E-06. Sample per second: 30604\n",
      "SLIM_BPR_Recommender: Epoch 23 of 50. Elapsed time 2.18 sec\n",
      "Processed 7947 ( 100.00% ) in 0.30 seconds. BPR loss is 8.01E-06. Sample per second: 26341\n",
      "SLIM_BPR_Recommender: Epoch 24 of 50. Elapsed time 2.22 sec\n",
      "Processed 7947 ( 100.00% ) in 0.33 seconds. BPR loss is 8.62E-06. Sample per second: 23883\n",
      "SLIM_BPR_Recommender: Epoch 25 of 50. Elapsed time 2.25 sec\n",
      "Processed 7947 ( 100.00% ) in 0.37 seconds. BPR loss is 1.08E-05. Sample per second: 21487\n",
      "SLIM_BPR_Recommender: Epoch 26 of 50. Elapsed time 2.29 sec\n",
      "Processed 7947 ( 100.00% ) in 0.40 seconds. BPR loss is 1.04E-05. Sample per second: 19646\n",
      "SLIM_BPR_Recommender: Epoch 27 of 50. Elapsed time 2.32 sec\n",
      "Processed 7947 ( 100.00% ) in 0.44 seconds. BPR loss is 1.14E-05. Sample per second: 18033\n",
      "SLIM_BPR_Recommender: Epoch 28 of 50. Elapsed time 2.36 sec\n",
      "Processed 7947 ( 100.00% ) in 0.48 seconds. BPR loss is 1.17E-05. Sample per second: 16643\n",
      "SLIM_BPR_Recommender: Epoch 29 of 50. Elapsed time 2.39 sec\n",
      "Processed 7947 ( 100.00% ) in 0.55 seconds. BPR loss is 1.27E-05. Sample per second: 14397\n",
      "SLIM_BPR_Recommender: Epoch 30 of 50. Elapsed time 2.47 sec\n",
      "Processed 7947 ( 100.00% ) in 0.60 seconds. BPR loss is 1.30E-05. Sample per second: 13210\n",
      "SLIM_BPR_Recommender: Epoch 31 of 50. Elapsed time 2.52 sec\n",
      "Processed 7947 ( 100.00% ) in 0.67 seconds. BPR loss is 1.58E-05. Sample per second: 11932\n",
      "SLIM_BPR_Recommender: Epoch 32 of 50. Elapsed time 2.58 sec\n",
      "Processed 7947 ( 100.00% ) in 0.72 seconds. BPR loss is 1.49E-05. Sample per second: 11115\n",
      "SLIM_BPR_Recommender: Epoch 33 of 50. Elapsed time 2.63 sec\n",
      "Processed 7947 ( 100.00% ) in 0.80 seconds. BPR loss is 1.63E-05. Sample per second: 9991\n",
      "SLIM_BPR_Recommender: Epoch 34 of 50. Elapsed time 2.71 sec\n",
      "Processed 7947 ( 100.00% ) in 0.89 seconds. BPR loss is 1.61E-05. Sample per second: 8924\n",
      "SLIM_BPR_Recommender: Epoch 35 of 50. Elapsed time 2.81 sec\n",
      "Processed 7947 ( 100.00% ) in 0.94 seconds. BPR loss is 1.67E-05. Sample per second: 8458\n",
      "SLIM_BPR_Recommender: Epoch 36 of 50. Elapsed time 2.86 sec\n",
      "Processed 7947 ( 100.00% ) in 1.02 seconds. BPR loss is 1.95E-05. Sample per second: 7790\n",
      "SLIM_BPR_Recommender: Epoch 37 of 50. Elapsed time 2.94 sec\n",
      "Processed 7947 ( 100.00% ) in 0.13 seconds. BPR loss is 1.89E-05. Sample per second: 60831\n",
      "SLIM_BPR_Recommender: Epoch 38 of 50. Elapsed time 3.05 sec\n",
      "Processed 7947 ( 100.00% ) in 0.24 seconds. BPR loss is 2.04E-05. Sample per second: 33257\n",
      "SLIM_BPR_Recommender: Epoch 39 of 50. Elapsed time 3.15 sec\n",
      "Processed 7947 ( 100.00% ) in 0.28 seconds. BPR loss is 2.07E-05. Sample per second: 28129\n",
      "SLIM_BPR_Recommender: Epoch 40 of 50. Elapsed time 3.20 sec\n",
      "Processed 7947 ( 100.00% ) in 0.32 seconds. BPR loss is 2.37E-05. Sample per second: 24735\n",
      "SLIM_BPR_Recommender: Epoch 41 of 50. Elapsed time 3.24 sec\n",
      "Processed 7947 ( 100.00% ) in 0.36 seconds. BPR loss is 2.37E-05. Sample per second: 21969\n",
      "SLIM_BPR_Recommender: Epoch 42 of 50. Elapsed time 3.28 sec\n",
      "Processed 7947 ( 100.00% ) in 0.41 seconds. BPR loss is 2.48E-05. Sample per second: 19212\n",
      "SLIM_BPR_Recommender: Epoch 43 of 50. Elapsed time 3.33 sec\n",
      "Processed 7947 ( 100.00% ) in 0.46 seconds. BPR loss is 2.54E-05. Sample per second: 17110\n",
      "SLIM_BPR_Recommender: Epoch 44 of 50. Elapsed time 3.40 sec\n",
      "Processed 7947 ( 100.00% ) in 0.52 seconds. BPR loss is 2.56E-05. Sample per second: 15284\n",
      "SLIM_BPR_Recommender: Epoch 45 of 50. Elapsed time 3.44 sec\n",
      "Processed 7947 ( 100.00% ) in 0.58 seconds. BPR loss is 3.00E-05. Sample per second: 13802\n",
      "SLIM_BPR_Recommender: Epoch 46 of 50. Elapsed time 3.49 sec\n",
      "Processed 7947 ( 100.00% ) in 0.62 seconds. BPR loss is 3.22E-05. Sample per second: 12779\n",
      "SLIM_BPR_Recommender: Epoch 47 of 50. Elapsed time 3.54 sec\n",
      "Processed 7947 ( 100.00% ) in 0.67 seconds. BPR loss is 3.15E-05. Sample per second: 11799\n",
      "SLIM_BPR_Recommender: Epoch 48 of 50. Elapsed time 3.59 sec\n",
      "Processed 7947 ( 100.00% ) in 0.71 seconds. BPR loss is 3.28E-05. Sample per second: 11135\n",
      "SLIM_BPR_Recommender: Epoch 49 of 50. Elapsed time 3.63 sec\n",
      "Processed 7947 ( 100.00% ) in 0.75 seconds. BPR loss is 3.51E-05. Sample per second: 10529\n",
      "SLIM_BPR_Recommender: Epoch 50 of 50. Elapsed time 3.67 sec\n"
     ]
    }
   ],
   "source": [
    "from SLIM_BPR.Cython.SLIM_BPR_Cython import SLIM_BPR_Cython\n",
    "\n",
    "\n",
    "\n",
    "recommender = SLIM_BPR_Cython(URM_train, recompile_cython=False)\n",
    "\n",
    "recommender.fit(epochs=50, batch_size=1, sgd_mode='sgd', learning_rate=1e-4, positive_threshold_BPR=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Processed 5607 ( 100.00% ) in 2.91 sec. Users per second: 1927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({5: {'ROC_AUC': 0.09944414719695624,\n",
       "   'PRECISION': 0.03777421080791838,\n",
       "   'PRECISION_RECALL_MIN_DEN': 0.07408299149872129,\n",
       "   'RECALL': 0.0661762215840626,\n",
       "   'MAP': 0.0441943602243227,\n",
       "   'MRR': 0.0941709767552461,\n",
       "   'NDCG': 0.057549240640812224,\n",
       "   'F1': 0.04809512547599055,\n",
       "   'HIT_RATE': 0.18887105403959337,\n",
       "   'ARHR': 0.10281196123892751,\n",
       "   'NOVELTY': 0.002398388748912338,\n",
       "   'AVERAGE_POPULARITY': 0.1316498590143871,\n",
       "   'DIVERSITY_MEAN_INTER_LIST': 0.9852515404111407,\n",
       "   'DIVERSITY_HERFINDAHL': 0.9970151644567453,\n",
       "   'COVERAGE_ITEM': 0.25570741097208854,\n",
       "   'COVERAGE_ITEM_CORRECT': 0.01832531280076997,\n",
       "   'COVERAGE_USER': 0.7055492638731596,\n",
       "   'COVERAGE_USER_CORRECT': 0.11350195042154272,\n",
       "   'DIVERSITY_GINI': 0.08708712420003395,\n",
       "   'SHANNON_ENTROPY': 10.693234120895994}},\n",
       " 'CUTOFF: 5 - ROC_AUC: 0.0994441, PRECISION: 0.0377742, PRECISION_RECALL_MIN_DEN: 0.0740830, RECALL: 0.0661762, MAP: 0.0441944, MRR: 0.0941710, NDCG: 0.0575492, F1: 0.0480951, HIT_RATE: 0.1888711, ARHR: 0.1028120, NOVELTY: 0.0023984, AVERAGE_POPULARITY: 0.1316499, DIVERSITY_MEAN_INTER_LIST: 0.9852515, DIVERSITY_HERFINDAHL: 0.9970152, COVERAGE_ITEM: 0.2557074, COVERAGE_ITEM_CORRECT: 0.0183253, COVERAGE_USER: 0.7055493, COVERAGE_USER_CORRECT: 0.1135020, DIVERSITY_GINI: 0.0870871, SHANNON_ENTROPY: 10.6932341, \\n')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_test, cutoff_list=[5])\n",
    "\n",
    "\n",
    "evaluator_validation.evaluateRecommender(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: URM Detected 1079 (4.15 %) cold items.\n",
      "Deallocating Cython objects\n",
      "Unable to read memory status: list index out of range\n",
      "SLIM_BPR_Recommender: Automatic selection of fastest train mode. Unable to get current RAM status, you may be using a non-Linux operating system. Using dense matrix.\n",
      "Processed 7947 ( 100.00% ) in 0.41 seconds. BPR loss is 2.59E-08. Sample per second: 19553\n",
      "SLIM_BPR_Recommender: Epoch 1 of 10. Elapsed time 0.02 sec\n",
      "Processed 7947 ( 100.00% ) in 0.43 seconds. BPR loss is 1.15E-07. Sample per second: 18271\n",
      "SLIM_BPR_Recommender: Epoch 2 of 10. Elapsed time 0.05 sec\n",
      "Processed 7947 ( 100.00% ) in 0.47 seconds. BPR loss is 2.34E-07. Sample per second: 16952\n",
      "SLIM_BPR_Recommender: Epoch 3 of 10. Elapsed time 0.09 sec\n",
      "Processed 7947 ( 100.00% ) in 0.50 seconds. BPR loss is 4.34E-07. Sample per second: 15788\n",
      "SLIM_BPR_Recommender: Epoch 4 of 10. Elapsed time 0.12 sec\n",
      "Processed 7947 ( 100.00% ) in 0.55 seconds. BPR loss is 6.44E-07. Sample per second: 14362\n",
      "SLIM_BPR_Recommender: Epoch 5 of 10. Elapsed time 0.17 sec\n",
      "Processed 7947 ( 100.00% ) in 0.58 seconds. BPR loss is 8.51E-07. Sample per second: 13598\n",
      "SLIM_BPR_Recommender: Epoch 6 of 10. Elapsed time 0.20 sec\n",
      "Processed 7947 ( 100.00% ) in 0.62 seconds. BPR loss is 1.04E-06. Sample per second: 12859\n",
      "SLIM_BPR_Recommender: Epoch 7 of 10. Elapsed time 0.23 sec\n",
      "Processed 7947 ( 100.00% ) in 0.65 seconds. BPR loss is 1.69E-06. Sample per second: 12310\n",
      "SLIM_BPR_Recommender: Epoch 8 of 10. Elapsed time 0.26 sec\n",
      "Processed 7947 ( 100.00% ) in 0.68 seconds. BPR loss is 1.94E-06. Sample per second: 11754\n",
      "SLIM_BPR_Recommender: Epoch 9 of 10. Elapsed time 0.29 sec\n",
      "Processed 7947 ( 100.00% ) in 0.70 seconds. BPR loss is 2.27E-06. Sample per second: 11327\n",
      "SLIM_BPR_Recommender: Epoch 10 of 10. Elapsed time 0.32 sec\n",
      "SLIM_BPR_Recommender: Terminating at epoch 10. Elapsed time 12.13 sec\n",
      "Deallocating Cython objects\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIM_BPR_Cython(URM_all, recompile_cython=False)\n",
    "\n",
    "recommender.fit(epochs=10, batch_size=1, sgd_mode='sgd', learning_rate=1e-4, positive_threshold_BPR=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoms = recommender.recommend(userTestList, cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomList = []\n",
    "for i in range(len(recoms)):\n",
    "    recomList.append(' '.join(str(e) for e in recoms[i]))\n",
    "# print(recoms)\n",
    "\n",
    "res = {\"user_id\": userTestList, \"item_list\": recomList}\n",
    "result = pd.DataFrame(res, columns= ['user_id', 'item_list'])\n",
    "\n",
    "result.to_csv ('outputs/slim_brp1.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
