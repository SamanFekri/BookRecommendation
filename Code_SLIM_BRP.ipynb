{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy.sparse as sps\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"./input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RM_train=pd.read_csv('./input/data_train.csv')\n",
    "R_test=pd.read_csv('./input/data_target_users_test.csv')\n",
    "URM=pd.read_csv('./input/data_train.csv')\n",
    "ICM = pd.read_csv('./input/data_ICM_title_abstract.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## URM all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_tuples = [tuple(x) for x in URM.to_numpy()]\n",
    "\n",
    "userList, itemList, ratingList = zip(*URM_tuples)\n",
    "\n",
    "userList = list(userList)\n",
    "userList=np.array(userList,dtype=np.int64)\n",
    "itemList = list(itemList)\n",
    "itemList=np.array(itemList,dtype=np.int64)\n",
    "\n",
    "ratingList = list(ratingList)                        #not needed\n",
    "ratingList=np.array(ratingList,dtype=np.int64)       #not needed\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all = URM_all.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items\t 24896, Number of users\t 7947\n",
      "Number of Intraction \t 113268\n",
      "Max ID items\t 25974, Max Id users\t 7946\n",
      "\n",
      "Average interactions per user 14.25\n",
      "Average interactions per item 4.55\n",
      "\n",
      "Sparsity 99.94 %\n"
     ]
    }
   ],
   "source": [
    "userList_unique = list(set(userList))\n",
    "itemList_unique = list(set(itemList))\n",
    "\n",
    "numUsers = len(userList_unique)\n",
    "numItems = len(itemList_unique)\n",
    "\n",
    "numberInteractions= len(URM_tuples)\n",
    "print (\"Number of items\\t {}, Number of users\\t {}\".format(numItems, numUsers))\n",
    "print(\"Number of Intraction \\t {}\" .format(numberInteractions))\n",
    "print (\"Max ID items\\t {}, Max Id users\\t {}\\n\".format(max(itemList_unique), max(userList_unique)))\n",
    "print (\"Average interactions per user {:.2f}\".format(numberInteractions/numUsers))\n",
    "print (\"Average interactions per item {:.2f}\\n\".format(numberInteractions/numItems))\n",
    "\n",
    "print (\"Sparsity {:.2f} %\".format((1-float(numberInteractions)/(numItems*numUsers))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ICM all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25975x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 490691 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ICM_tuples = [tuple(x) for x in ICM.to_numpy()]\n",
    "itemList_icm, featureList_icm, scoreList_icm = zip(*ICM_tuples)\n",
    "\n",
    "itemList_icm = list(itemList_icm)\n",
    "itemList_icm = np.array(itemList_icm,dtype=np.int64)\n",
    "\n",
    "featureList_icm = list(featureList_icm)\n",
    "featureList_icm = np.array(featureList_icm,dtype=np.int64)\n",
    "\n",
    "scoreList_icm = list(scoreList_icm)\n",
    "scoreList_icm = np.array(scoreList_icm,dtype=np.float64)\n",
    "\n",
    "ICM_all = sps.coo_matrix((scoreList_icm, (itemList_icm, featureList_icm)))\n",
    "\n",
    "ICM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userTestList = [x for x in R_test.to_numpy()]\n",
    "userTestList = zip(*userTestList)\n",
    "userTestList = [list(a) for a in userTestList][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 73 (0.92 %) of 7947 users have no train items\n",
      "Warning: 2277 (28.65 %) of 7947 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.80)\n",
    "# URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train, train_percentage = 0.80)\n",
    "\n",
    "# evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement SLIM BPR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Sampling\n",
    "\n",
    "#### Create a mask of positive interactions. How to build it depends on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7947x25975 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 90614 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_mask = URM_train.copy()\n",
    "URM_mask.data[URM_mask.data < 1] = 0\n",
    "\n",
    "URM_mask.eliminate_zeros()\n",
    "URM_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = URM_mask.shape[0]\n",
    "n_items = URM_mask.shape[1]\n",
    "\n",
    "\n",
    "# Extract users having at least one interaction to choose from\n",
    "eligibleUsers = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "    start_pos = URM_mask.indptr[user_id]\n",
    "    end_pos = URM_mask.indptr[user_id+1]\n",
    "    if len(URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "        eligibleUsers.append(user_id)\n",
    "                \n",
    "\n",
    "def sampleTriplet():\n",
    "    \n",
    "    # By randomly selecting a user in this way we could end up \n",
    "    # with a user with no interactions\n",
    "    #user_id = np.random.randint(0, n_users)\n",
    "    \n",
    "    user_id = np.random.choice(eligibleUsers)\n",
    "    \n",
    "    # Get user seen items and choose one\n",
    "    userSeenItems = URM_mask[user_id,:].indices\n",
    "    pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "    negItemSelected = False\n",
    "\n",
    "    # It's faster to just try again then to build a mapping of the non-seen items\n",
    "    while (not negItemSelected):\n",
    "        neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "        if (neg_item_id not in userSeenItems):\n",
    "            \n",
    "            negItemSelected = True\n",
    "\n",
    "    return user_id, pos_item_id, neg_item_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(589, 22122, 4434)\n",
      "(276, 5082, 652)\n",
      "(7111, 10418, 9617)\n",
      "(3555, 8955, 20438)\n",
      "(4178, 14245, 8291)\n",
      "(4096, 7260, 21525)\n",
      "(3003, 10412, 12832)\n",
      "(5315, 10452, 10611)\n",
      "(511, 13296, 13726)\n",
      "(3371, 17739, 15359)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampleTriplet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Computing prediction\n",
    "\n",
    "#### The prediction depends on the model: SLIM, Matrix Factorization... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to initialize our model. In case of SLIM it works best to initialize S as zero, in case of MF you cannot because of how the gradient is computed and you have to initialize at random. Here we initialize SLIM at random just so that we have some numbers to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.random.random((n_items,n_items))\n",
    "similarity_matrix[np.arange(n_items),np.arange(n_items)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, positive_item_id, negative_item_id = sampleTriplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12873"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7030, 12873, 14969, 18945], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userSeenItems = URM_mask[user_id,:].indices\n",
    "userSeenItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i is 2.03, x_j is 1.17\n"
     ]
    }
   ],
   "source": [
    "x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "print(\"x_i is {:.2f}, x_j is {:.2f}\".format(x_i, x_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Computing gradient\n",
    "\n",
    "#### The gradient depends on the objective function: RMSE, BPR... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8562944394656526"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ij = x_i - x_j\n",
    "x_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The original BPR paper uses the logarithm of the sigmoid of x_ij, whose derivative is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2981141243992883"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = 1 / (1 + np.exp(x_ij))\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Update model\n",
    "\n",
    "#### How to update depends on the model itself, here we have just one paramether, the similarity matrix, so we perform just one update. In matrix factorization we have two.\n",
    "\n",
    "#### We need a learning rate, which influences how fast the model will change. Small ones lead to slower convergence but often higher results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "similarity_matrix[negative_item_id, negative_item_id] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usually there is no relevant change in the scores over a single iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i is 2.03, x_j is 1.17\n"
     ]
    }
   ],
   "source": [
    "x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "print(\"x_i is {:.2f}, x_j is {:.2f}\".format(x_i, x_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Write the iterative epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epochIteration():\n",
    "\n",
    "    # Get number of available interactions\n",
    "    numPositiveIteractions = int(URM_mask.nnz*0.01)\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    start_time_batch = time.time()\n",
    "\n",
    "    # Uniform user sampling without replacement\n",
    "    for num_sample in range(numPositiveIteractions):\n",
    "\n",
    "        # Sample\n",
    "        user_id, positive_item_id, negative_item_id = sampleTriplet()\n",
    "        \n",
    "        userSeenItems = URM_mask[user_id,:].indices\n",
    "        \n",
    "        # Prediction\n",
    "        x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "        x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "        \n",
    "        # Gradient\n",
    "        x_ij = x_i - x_j\n",
    "\n",
    "        gradient = 1 / (1 + np.exp(x_ij))\n",
    "        \n",
    "        # Update\n",
    "        similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "        similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "        similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "        similarity_matrix[negative_item_id, negative_item_id] = 0\n",
    "        \n",
    "\n",
    "        if(time.time() - start_time_batch >= 30 or num_sample == numPositiveIteractions-1):\n",
    "            print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                num_sample,\n",
    "                100.0* float(num_sample)/numPositiveIteractions,\n",
    "                time.time() - start_time_batch,\n",
    "                float(num_sample) / (time.time() - start_time_epoch)))\n",
    "\n",
    "\n",
    "            start_time_batch = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 905 ( 99.89% ) in 1.64 seconds. Sample per second: 553\n"
     ]
    }
   ],
   "source": [
    "epochIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "def similarityMatrixTopK(item_weights, forceSparseOutput = True, k=100, verbose = False, inplace=True):\n",
    "    \"\"\"\n",
    "    The function selects the TopK most similar elements, column-wise\n",
    "\n",
    "    :param item_weights:\n",
    "    :param forceSparseOutput:\n",
    "    :param k:\n",
    "    :param verbose:\n",
    "    :param inplace: Default True, WARNING matrix will be modified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating topK matrix\")\n",
    "\n",
    "    nitems = item_weights.shape[1]\n",
    "    k = min(k, nitems)\n",
    "\n",
    "    # for each column, keep only the top-k scored items\n",
    "    sparse_weights = not isinstance(item_weights, np.ndarray)\n",
    "\n",
    "    if not sparse_weights:\n",
    "\n",
    "        idx_sorted = np.argsort(item_weights, axis=0)  # sort data inside each column\n",
    "\n",
    "        if inplace:\n",
    "            W = item_weights\n",
    "        else:\n",
    "            W = item_weights.copy()\n",
    "\n",
    "        # index of the items that don't belong to the top-k similar items of each column\n",
    "        not_top_k = idx_sorted[:-k, :]\n",
    "        # use numpy fancy indexing to zero-out the values in sim without using a for loop\n",
    "        W[not_top_k, np.arange(nitems)] = 0.0\n",
    "\n",
    "        if forceSparseOutput:\n",
    "            W_sparse = sps.csr_matrix(W, shape=(nitems, nitems))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dense TopK matrix generated in {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "        return W\n",
    "\n",
    "    else:\n",
    "        # iterate over each column and keep only the top-k similar items\n",
    "        data, rows_indices, cols_indptr = [], [], []\n",
    "\n",
    "        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n",
    "\n",
    "        for item_idx in range(nitems):\n",
    "\n",
    "            cols_indptr.append(len(data))\n",
    "\n",
    "            start_position = item_weights.indptr[item_idx]\n",
    "            end_position = item_weights.indptr[item_idx+1]\n",
    "\n",
    "            column_data = item_weights.data[start_position:end_position]\n",
    "            column_row_index = item_weights.indices[start_position:end_position]\n",
    "\n",
    "            non_zero_data = column_data!=0\n",
    "\n",
    "            idx_sorted = np.argsort(column_data[non_zero_data])  # sort by column\n",
    "            top_k_idx = idx_sorted[-k:]\n",
    "\n",
    "            data.extend(column_data[non_zero_data][top_k_idx])\n",
    "            rows_indices.extend(column_row_index[non_zero_data][top_k_idx])\n",
    "\n",
    "\n",
    "        cols_indptr.append(len(data))\n",
    "\n",
    "        # During testing CSR is faster\n",
    "        W_sparse = sps.csc_matrix((data, rows_indices, cols_indptr), shape=(nitems, nitems), dtype=np.float32)\n",
    "        W_sparse = W_sparse.tocsr()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "        return W_sparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SLIM_BPR_Recommender(object):\n",
    "    \"\"\" SLIM_BPR recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM):\n",
    "        self.URM = URM\n",
    "        \n",
    "        self.similarity_matrix = np.zeros((n_items,n_items))\n",
    "        \n",
    "        self.URM_mask = self.URM.copy()\n",
    "        self.URM_mask.data[self.URM_mask.data < 1] = 0\n",
    "        self.URM_mask.eliminate_zeros()\n",
    "        \n",
    "        self.n_users = URM_mask.shape[0]\n",
    "        self.n_items = URM_mask.shape[1]\n",
    "\n",
    "\n",
    "        # Extract users having at least one interaction to choose from\n",
    "        self.eligibleUsers = []\n",
    "\n",
    "        for user_id in range(n_users):\n",
    "\n",
    "            start_pos = self.URM_mask.indptr[user_id]\n",
    "            end_pos = self.URM_mask.indptr[user_id+1]\n",
    "\n",
    "            if len(self.URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "                self.eligibleUsers.append(user_id)\n",
    "\n",
    "\n",
    "\n",
    "    def sampleTriplet(self):\n",
    "\n",
    "        # By randomly selecting a user in this way we could end up \n",
    "        # with a user with no interactions\n",
    "        #user_id = np.random.randint(0, n_users)\n",
    "\n",
    "        user_id = np.random.choice(self.eligibleUsers)\n",
    "\n",
    "        # Get user seen items and choose one\n",
    "        userSeenItems = URM_mask[user_id,:].indices\n",
    "        pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "        negItemSelected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        while (not negItemSelected):\n",
    "            neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "            if (neg_item_id not in userSeenItems):\n",
    "\n",
    "                negItemSelected = True\n",
    "\n",
    "        return user_id, pos_item_id, neg_item_id\n",
    "\n",
    "\n",
    "        \n",
    "    def epochIteration(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        numPositiveIteractions = int(self.URM_mask.nnz*0.01)\n",
    "\n",
    "        start_time_epoch = time.time()\n",
    "        start_time_batch = time.time()\n",
    "\n",
    "        # Uniform user sampling without replacement\n",
    "        for num_sample in range(numPositiveIteractions):\n",
    "\n",
    "            # Sample\n",
    "            user_id, positive_item_id, negative_item_id = self.sampleTriplet()\n",
    "\n",
    "            userSeenItems = self.URM_mask[user_id,:].indices\n",
    "\n",
    "            # Prediction\n",
    "            x_i = self.similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "            x_j = self.similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "            # Gradient\n",
    "            x_ij = x_i - x_j\n",
    "\n",
    "            gradient = 1 / (1 + np.exp(x_ij))\n",
    "\n",
    "            # Update\n",
    "            self.similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "            self.similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "            self.similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "            self.similarity_matrix[negative_item_id, negative_item_id] = 0\n",
    "\n",
    "\n",
    "            if(time.time() - start_time_batch >= 30 or num_sample == numPositiveIteractions-1):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                    num_sample,\n",
    "                    100.0* float(num_sample)/numPositiveIteractions,\n",
    "                    time.time() - start_time_batch,\n",
    "                    float(num_sample) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                start_time_batch = time.time()\n",
    "\n",
    "                \n",
    "    def fit(self, learning_rate = 0.01, epochs = 10):\n",
    " \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        for numEpoch in range(self.epochs):\n",
    "            self.epochIteration()\n",
    "            \n",
    "        self.similarity_matrix = self.similarity_matrix.T\n",
    "        \n",
    "        self.similarity_matrix = similarityMatrixTopK(self.similarity_matrix, k=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.similarity_matrix).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 905 ( 99.89% ) in 0.96 seconds. Sample per second: 947\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIM_BPR_Recommender(URM_train)\n",
    "recommender.fit(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated user 0 of 7947\n",
      "Recommender performance is: Precision = 0.0049, Recall = 0.0062, MAP = 0.0041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.004867724867724857,\n",
       " 'recall': 0.006205754098470564,\n",
       " 'MAP': 0.0041314912796394255}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Notebooks_utils.evaluation_function import evaluate_algorithm\n",
    "\n",
    "evaluate_algorithm(URM_test, recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: URM Detected 73 (0.92 %) cold users.\n",
      "SLIM_BPR_Recommender: URM Detected 2373 (9.14 %) cold items.\n",
      "Unable to read memory status: list index out of range\n",
      "SLIM_BPR_Recommender: Automatic selection of fastest train mode. Unable to get current RAM status, you may be using a non-Linux operating system. Using dense matrix.\n",
      "Processed 7947 ( 100.00% ) in 0.56 seconds. BPR loss is 3.22E-08. Sample per second: 14197\n",
      "SLIM_BPR_Recommender: Epoch 1 of 100. Elapsed time 0.02 sec\n",
      "Processed 7947 ( 100.00% ) in 0.58 seconds. BPR loss is 8.64E-08. Sample per second: 13651\n",
      "SLIM_BPR_Recommender: Epoch 2 of 100. Elapsed time 0.04 sec\n",
      "Processed 7947 ( 100.00% ) in 0.61 seconds. BPR loss is 1.64E-07. Sample per second: 13135\n",
      "SLIM_BPR_Recommender: Epoch 3 of 100. Elapsed time 0.06 sec\n",
      "Processed 7947 ( 100.00% ) in 0.63 seconds. BPR loss is 2.59E-07. Sample per second: 12651\n",
      "SLIM_BPR_Recommender: Epoch 4 of 100. Elapsed time 0.09 sec\n",
      "Processed 7947 ( 100.00% ) in 0.65 seconds. BPR loss is 3.92E-07. Sample per second: 12235\n",
      "SLIM_BPR_Recommender: Epoch 5 of 100. Elapsed time 0.11 sec\n",
      "Processed 7947 ( 100.00% ) in 0.67 seconds. BPR loss is 5.71E-07. Sample per second: 11879\n",
      "SLIM_BPR_Recommender: Epoch 6 of 100. Elapsed time 0.13 sec\n",
      "Processed 7947 ( 100.00% ) in 0.69 seconds. BPR loss is 7.53E-07. Sample per second: 11507\n",
      "SLIM_BPR_Recommender: Epoch 7 of 100. Elapsed time 0.15 sec\n",
      "Processed 7947 ( 100.00% ) in 0.71 seconds. BPR loss is 1.02E-06. Sample per second: 11145\n",
      "SLIM_BPR_Recommender: Epoch 8 of 100. Elapsed time 0.17 sec\n",
      "Processed 7947 ( 100.00% ) in 0.73 seconds. BPR loss is 1.26E-06. Sample per second: 10817\n",
      "SLIM_BPR_Recommender: Epoch 9 of 100. Elapsed time 0.19 sec\n",
      "Processed 7947 ( 100.00% ) in 0.75 seconds. BPR loss is 1.50E-06. Sample per second: 10534\n",
      "SLIM_BPR_Recommender: Epoch 10 of 100. Elapsed time 0.21 sec\n",
      "Processed 7947 ( 100.00% ) in 0.77 seconds. BPR loss is 1.77E-06. Sample per second: 10272\n",
      "SLIM_BPR_Recommender: Epoch 11 of 100. Elapsed time 0.23 sec\n",
      "Processed 7947 ( 100.00% ) in 0.79 seconds. BPR loss is 2.21E-06. Sample per second: 10014\n",
      "SLIM_BPR_Recommender: Epoch 12 of 100. Elapsed time 0.25 sec\n",
      "Processed 7947 ( 100.00% ) in 0.82 seconds. BPR loss is 2.44E-06. Sample per second: 9749\n",
      "SLIM_BPR_Recommender: Epoch 13 of 100. Elapsed time 0.27 sec\n",
      "Processed 7947 ( 100.00% ) in 0.84 seconds. BPR loss is 3.20E-06. Sample per second: 9490\n",
      "SLIM_BPR_Recommender: Epoch 14 of 100. Elapsed time 0.29 sec\n",
      "Processed 7947 ( 100.00% ) in 0.86 seconds. BPR loss is 3.25E-06. Sample per second: 9263\n",
      "SLIM_BPR_Recommender: Epoch 15 of 100. Elapsed time 0.31 sec\n",
      "Processed 7947 ( 100.00% ) in 0.88 seconds. BPR loss is 3.72E-06. Sample per second: 9050\n",
      "SLIM_BPR_Recommender: Epoch 16 of 100. Elapsed time 0.34 sec\n",
      "Processed 7947 ( 100.00% ) in 0.90 seconds. BPR loss is 4.21E-06. Sample per second: 8846\n",
      "SLIM_BPR_Recommender: Epoch 17 of 100. Elapsed time 0.36 sec\n",
      "Processed 7947 ( 100.00% ) in 0.92 seconds. BPR loss is 4.83E-06. Sample per second: 8664\n",
      "SLIM_BPR_Recommender: Epoch 18 of 100. Elapsed time 0.37 sec\n",
      "Processed 7947 ( 100.00% ) in 0.94 seconds. BPR loss is 5.59E-06. Sample per second: 8466\n",
      "SLIM_BPR_Recommender: Epoch 19 of 100. Elapsed time 0.40 sec\n",
      "Processed 7947 ( 100.00% ) in 0.96 seconds. BPR loss is 5.90E-06. Sample per second: 8245\n",
      "SLIM_BPR_Recommender: Epoch 20 of 100. Elapsed time 0.42 sec\n",
      "Processed 7947 ( 100.00% ) in 0.98 seconds. BPR loss is 6.43E-06. Sample per second: 8077\n",
      "SLIM_BPR_Recommender: Epoch 21 of 100. Elapsed time 0.44 sec\n",
      "Processed 7947 ( 100.00% ) in 1.01 seconds. BPR loss is 7.93E-06. Sample per second: 7900\n",
      "SLIM_BPR_Recommender: Epoch 22 of 100. Elapsed time 0.46 sec\n",
      "Processed 7947 ( 100.00% ) in 0.03 seconds. BPR loss is 7.58E-06. Sample per second: 291690\n",
      "SLIM_BPR_Recommender: Epoch 23 of 100. Elapsed time 0.48 sec\n",
      "Processed 7947 ( 100.00% ) in 0.05 seconds. BPR loss is 7.85E-06. Sample per second: 173000\n",
      "SLIM_BPR_Recommender: Epoch 24 of 100. Elapsed time 0.50 sec\n",
      "Processed 7947 ( 100.00% ) in 0.07 seconds. BPR loss is 9.64E-06. Sample per second: 119957\n",
      "SLIM_BPR_Recommender: Epoch 25 of 100. Elapsed time 0.52 sec\n",
      "Processed 7947 ( 100.00% ) in 0.09 seconds. BPR loss is 9.51E-06. Sample per second: 92360\n",
      "SLIM_BPR_Recommender: Epoch 26 of 100. Elapsed time 0.54 sec\n",
      "Processed 7947 ( 100.00% ) in 0.11 seconds. BPR loss is 1.04E-05. Sample per second: 72080\n",
      "SLIM_BPR_Recommender: Epoch 27 of 100. Elapsed time 0.57 sec\n",
      "Processed 7947 ( 100.00% ) in 0.13 seconds. BPR loss is 1.12E-05. Sample per second: 59732\n",
      "SLIM_BPR_Recommender: Epoch 28 of 100. Elapsed time 0.59 sec\n",
      "Processed 7947 ( 100.00% ) in 0.15 seconds. BPR loss is 1.16E-05. Sample per second: 51497\n",
      "SLIM_BPR_Recommender: Epoch 29 of 100. Elapsed time 0.61 sec\n",
      "Processed 7947 ( 100.00% ) in 0.17 seconds. BPR loss is 1.23E-05. Sample per second: 45522\n",
      "SLIM_BPR_Recommender: Epoch 30 of 100. Elapsed time 0.63 sec\n",
      "Processed 7947 ( 100.00% ) in 0.20 seconds. BPR loss is 1.42E-05. Sample per second: 40607\n",
      "SLIM_BPR_Recommender: Epoch 31 of 100. Elapsed time 0.65 sec\n",
      "Processed 7947 ( 100.00% ) in 0.22 seconds. BPR loss is 1.42E-05. Sample per second: 36269\n",
      "SLIM_BPR_Recommender: Epoch 32 of 100. Elapsed time 0.68 sec\n",
      "Processed 7947 ( 100.00% ) in 0.24 seconds. BPR loss is 1.58E-05. Sample per second: 33271\n",
      "SLIM_BPR_Recommender: Epoch 33 of 100. Elapsed time 0.70 sec\n",
      "Processed 7947 ( 100.00% ) in 0.26 seconds. BPR loss is 1.80E-05. Sample per second: 30780\n",
      "SLIM_BPR_Recommender: Epoch 34 of 100. Elapsed time 0.72 sec\n",
      "Processed 7947 ( 100.00% ) in 0.28 seconds. BPR loss is 1.80E-05. Sample per second: 28465\n",
      "SLIM_BPR_Recommender: Epoch 35 of 100. Elapsed time 0.74 sec\n",
      "Processed 7947 ( 100.00% ) in 0.30 seconds. BPR loss is 1.79E-05. Sample per second: 26559\n",
      "SLIM_BPR_Recommender: Epoch 36 of 100. Elapsed time 0.76 sec\n",
      "Processed 7947 ( 100.00% ) in 0.32 seconds. BPR loss is 1.85E-05. Sample per second: 24713\n",
      "SLIM_BPR_Recommender: Epoch 37 of 100. Elapsed time 0.78 sec\n",
      "Processed 7947 ( 100.00% ) in 0.35 seconds. BPR loss is 2.20E-05. Sample per second: 22947\n",
      "SLIM_BPR_Recommender: Epoch 38 of 100. Elapsed time 0.80 sec\n",
      "Processed 7947 ( 100.00% ) in 0.37 seconds. BPR loss is 2.09E-05. Sample per second: 21707\n",
      "SLIM_BPR_Recommender: Epoch 39 of 100. Elapsed time 0.82 sec\n",
      "Processed 7947 ( 100.00% ) in 0.39 seconds. BPR loss is 2.20E-05. Sample per second: 20571\n",
      "SLIM_BPR_Recommender: Epoch 40 of 100. Elapsed time 0.84 sec\n",
      "Processed 7947 ( 100.00% ) in 0.41 seconds. BPR loss is 2.52E-05. Sample per second: 19202\n",
      "SLIM_BPR_Recommender: Epoch 41 of 100. Elapsed time 0.87 sec\n",
      "Processed 7947 ( 100.00% ) in 0.43 seconds. BPR loss is 2.46E-05. Sample per second: 18339\n",
      "SLIM_BPR_Recommender: Epoch 42 of 100. Elapsed time 0.89 sec\n",
      "Processed 7947 ( 100.00% ) in 0.45 seconds. BPR loss is 2.62E-05. Sample per second: 17536\n",
      "SLIM_BPR_Recommender: Epoch 43 of 100. Elapsed time 0.91 sec\n",
      "Processed 7947 ( 100.00% ) in 0.47 seconds. BPR loss is 2.78E-05. Sample per second: 16765\n",
      "SLIM_BPR_Recommender: Epoch 44 of 100. Elapsed time 0.93 sec\n",
      "Processed 7947 ( 100.00% ) in 0.49 seconds. BPR loss is 2.69E-05. Sample per second: 16101\n",
      "SLIM_BPR_Recommender: Epoch 45 of 100. Elapsed time 0.95 sec\n",
      "Processed 7947 ( 100.00% ) in 0.51 seconds. BPR loss is 2.60E-05. Sample per second: 15441\n",
      "SLIM_BPR_Recommender: Epoch 46 of 100. Elapsed time 0.97 sec\n",
      "Processed 7947 ( 100.00% ) in 0.54 seconds. BPR loss is 3.36E-05. Sample per second: 14673\n",
      "SLIM_BPR_Recommender: Epoch 47 of 100. Elapsed time 1.00 sec\n",
      "Processed 7947 ( 100.00% ) in 0.56 seconds. BPR loss is 3.13E-05. Sample per second: 14152\n",
      "SLIM_BPR_Recommender: Epoch 48 of 100. Elapsed time 1.02 sec\n",
      "Processed 7947 ( 100.00% ) in 0.58 seconds. BPR loss is 3.37E-05. Sample per second: 13631\n",
      "SLIM_BPR_Recommender: Epoch 49 of 100. Elapsed time 1.04 sec\n",
      "Processed 7947 ( 100.00% ) in 0.61 seconds. BPR loss is 3.48E-05. Sample per second: 13135\n",
      "SLIM_BPR_Recommender: Epoch 50 of 100. Elapsed time 1.06 sec\n",
      "Processed 7947 ( 100.00% ) in 0.63 seconds. BPR loss is 3.61E-05. Sample per second: 12658\n",
      "SLIM_BPR_Recommender: Epoch 51 of 100. Elapsed time 1.09 sec\n",
      "Processed 7947 ( 100.00% ) in 0.65 seconds. BPR loss is 3.61E-05. Sample per second: 12221\n",
      "SLIM_BPR_Recommender: Epoch 52 of 100. Elapsed time 1.11 sec\n",
      "Processed 7947 ( 100.00% ) in 0.67 seconds. BPR loss is 4.01E-05. Sample per second: 11831\n",
      "SLIM_BPR_Recommender: Epoch 53 of 100. Elapsed time 1.13 sec\n",
      "Processed 7947 ( 100.00% ) in 0.70 seconds. BPR loss is 4.24E-05. Sample per second: 11432\n",
      "SLIM_BPR_Recommender: Epoch 54 of 100. Elapsed time 1.15 sec\n",
      "Processed 7947 ( 100.00% ) in 0.72 seconds. BPR loss is 3.95E-05. Sample per second: 11051\n",
      "SLIM_BPR_Recommender: Epoch 55 of 100. Elapsed time 1.18 sec\n",
      "Processed 7947 ( 100.00% ) in 0.75 seconds. BPR loss is 4.48E-05. Sample per second: 10611\n",
      "SLIM_BPR_Recommender: Epoch 56 of 100. Elapsed time 1.21 sec\n",
      "Processed 7947 ( 100.00% ) in 0.77 seconds. BPR loss is 4.28E-05. Sample per second: 10261\n",
      "SLIM_BPR_Recommender: Epoch 57 of 100. Elapsed time 1.23 sec\n",
      "Processed 7947 ( 100.00% ) in 0.80 seconds. BPR loss is 4.95E-05. Sample per second: 9980\n",
      "SLIM_BPR_Recommender: Epoch 58 of 100. Elapsed time 1.26 sec\n",
      "Processed 7947 ( 100.00% ) in 0.82 seconds. BPR loss is 4.86E-05. Sample per second: 9672\n",
      "SLIM_BPR_Recommender: Epoch 59 of 100. Elapsed time 1.29 sec\n",
      "Processed 7947 ( 100.00% ) in 0.85 seconds. BPR loss is 4.83E-05. Sample per second: 9371\n",
      "SLIM_BPR_Recommender: Epoch 60 of 100. Elapsed time 1.31 sec\n",
      "Processed 7947 ( 100.00% ) in 0.87 seconds. BPR loss is 5.60E-05. Sample per second: 9123\n",
      "SLIM_BPR_Recommender: Epoch 61 of 100. Elapsed time 1.33 sec\n",
      "Processed 7947 ( 100.00% ) in 0.90 seconds. BPR loss is 5.38E-05. Sample per second: 8873\n",
      "SLIM_BPR_Recommender: Epoch 62 of 100. Elapsed time 1.36 sec\n",
      "Processed 7947 ( 100.00% ) in 0.92 seconds. BPR loss is 5.31E-05. Sample per second: 8602\n",
      "SLIM_BPR_Recommender: Epoch 63 of 100. Elapsed time 1.38 sec\n",
      "Processed 7947 ( 100.00% ) in 0.95 seconds. BPR loss is 5.54E-05. Sample per second: 8330\n",
      "SLIM_BPR_Recommender: Epoch 64 of 100. Elapsed time 1.41 sec\n",
      "Processed 7947 ( 100.00% ) in 0.98 seconds. BPR loss is 5.59E-05. Sample per second: 8146\n",
      "SLIM_BPR_Recommender: Epoch 65 of 100. Elapsed time 1.43 sec\n",
      "Processed 7947 ( 100.00% ) in 1.00 seconds. BPR loss is 6.12E-05. Sample per second: 7958\n",
      "SLIM_BPR_Recommender: Epoch 66 of 100. Elapsed time 1.46 sec\n",
      "Processed 7947 ( 100.00% ) in 0.02 seconds. BPR loss is 6.11E-05. Sample per second: 358856\n",
      "SLIM_BPR_Recommender: Epoch 67 of 100. Elapsed time 1.48 sec\n",
      "Processed 7947 ( 100.00% ) in 0.05 seconds. BPR loss is 6.42E-05. Sample per second: 163233\n",
      "SLIM_BPR_Recommender: Epoch 68 of 100. Elapsed time 1.51 sec\n",
      "Processed 7947 ( 100.00% ) in 0.07 seconds. BPR loss is 6.73E-05. Sample per second: 106375\n",
      "SLIM_BPR_Recommender: Epoch 69 of 100. Elapsed time 1.53 sec\n",
      "Processed 7947 ( 100.00% ) in 0.10 seconds. BPR loss is 6.12E-05. Sample per second: 81936\n",
      "SLIM_BPR_Recommender: Epoch 70 of 100. Elapsed time 1.55 sec\n",
      "Processed 7947 ( 100.00% ) in 0.12 seconds. BPR loss is 6.49E-05. Sample per second: 67268\n",
      "SLIM_BPR_Recommender: Epoch 71 of 100. Elapsed time 1.57 sec\n",
      "Processed 7947 ( 100.00% ) in 0.14 seconds. BPR loss is 7.14E-05. Sample per second: 56101\n",
      "SLIM_BPR_Recommender: Epoch 72 of 100. Elapsed time 1.60 sec\n",
      "Processed 7947 ( 100.00% ) in 0.16 seconds. BPR loss is 7.17E-05. Sample per second: 48674\n",
      "SLIM_BPR_Recommender: Epoch 73 of 100. Elapsed time 1.62 sec\n",
      "Processed 7947 ( 100.00% ) in 0.18 seconds. BPR loss is 7.44E-05. Sample per second: 43355\n",
      "SLIM_BPR_Recommender: Epoch 74 of 100. Elapsed time 1.64 sec\n",
      "Processed 7947 ( 100.00% ) in 0.20 seconds. BPR loss is 7.87E-05. Sample per second: 38804\n",
      "SLIM_BPR_Recommender: Epoch 75 of 100. Elapsed time 1.66 sec\n",
      "Processed 7947 ( 100.00% ) in 0.23 seconds. BPR loss is 8.60E-05. Sample per second: 35230\n",
      "SLIM_BPR_Recommender: Epoch 76 of 100. Elapsed time 1.68 sec\n",
      "Processed 7947 ( 100.00% ) in 0.25 seconds. BPR loss is 8.09E-05. Sample per second: 32364\n",
      "SLIM_BPR_Recommender: Epoch 77 of 100. Elapsed time 1.70 sec\n",
      "Processed 7947 ( 100.00% ) in 0.26 seconds. BPR loss is 8.21E-05. Sample per second: 29993\n",
      "SLIM_BPR_Recommender: Epoch 78 of 100. Elapsed time 1.72 sec\n",
      "Processed 7947 ( 100.00% ) in 0.29 seconds. BPR loss is 9.04E-05. Sample per second: 27794\n",
      "SLIM_BPR_Recommender: Epoch 79 of 100. Elapsed time 1.74 sec\n",
      "Processed 7947 ( 100.00% ) in 0.31 seconds. BPR loss is 8.43E-05. Sample per second: 25920\n",
      "SLIM_BPR_Recommender: Epoch 80 of 100. Elapsed time 1.76 sec\n",
      "Processed 7947 ( 100.00% ) in 0.33 seconds. BPR loss is 9.21E-05. Sample per second: 24208\n",
      "SLIM_BPR_Recommender: Epoch 81 of 100. Elapsed time 1.79 sec\n",
      "Processed 7947 ( 100.00% ) in 0.35 seconds. BPR loss is 9.77E-05. Sample per second: 22763\n",
      "SLIM_BPR_Recommender: Epoch 82 of 100. Elapsed time 1.81 sec\n",
      "Processed 7947 ( 100.00% ) in 0.37 seconds. BPR loss is 9.91E-05. Sample per second: 21576\n",
      "SLIM_BPR_Recommender: Epoch 83 of 100. Elapsed time 1.83 sec\n",
      "Processed 7947 ( 100.00% ) in 0.39 seconds. BPR loss is 9.47E-05. Sample per second: 20456\n",
      "SLIM_BPR_Recommender: Epoch 84 of 100. Elapsed time 1.85 sec\n",
      "Processed 7947 ( 100.00% ) in 0.41 seconds. BPR loss is 1.09E-04. Sample per second: 19411\n",
      "SLIM_BPR_Recommender: Epoch 85 of 100. Elapsed time 1.87 sec\n",
      "Processed 7947 ( 100.00% ) in 0.43 seconds. BPR loss is 1.05E-04. Sample per second: 18430\n",
      "SLIM_BPR_Recommender: Epoch 86 of 100. Elapsed time 1.89 sec\n",
      "Processed 7947 ( 100.00% ) in 0.45 seconds. BPR loss is 1.08E-04. Sample per second: 17581\n",
      "SLIM_BPR_Recommender: Epoch 87 of 100. Elapsed time 1.91 sec\n",
      "Processed 7947 ( 100.00% ) in 0.47 seconds. BPR loss is 1.06E-04. Sample per second: 16799\n",
      "SLIM_BPR_Recommender: Epoch 88 of 100. Elapsed time 1.93 sec\n",
      "Processed 7947 ( 100.00% ) in 0.49 seconds. BPR loss is 1.20E-04. Sample per second: 16098\n",
      "SLIM_BPR_Recommender: Epoch 89 of 100. Elapsed time 1.95 sec\n",
      "Processed 7947 ( 100.00% ) in 0.52 seconds. BPR loss is 1.10E-04. Sample per second: 15235\n",
      "SLIM_BPR_Recommender: Epoch 90 of 100. Elapsed time 1.98 sec\n",
      "Processed 7947 ( 100.00% ) in 0.54 seconds. BPR loss is 1.19E-04. Sample per second: 14588\n",
      "SLIM_BPR_Recommender: Epoch 91 of 100. Elapsed time 2.00 sec\n",
      "Processed 7947 ( 100.00% ) in 0.57 seconds. BPR loss is 1.17E-04. Sample per second: 14045\n",
      "SLIM_BPR_Recommender: Epoch 92 of 100. Elapsed time 2.02 sec\n",
      "Processed 7947 ( 100.00% ) in 0.59 seconds. BPR loss is 1.23E-04. Sample per second: 13552\n",
      "SLIM_BPR_Recommender: Epoch 93 of 100. Elapsed time 2.04 sec\n",
      "Processed 7947 ( 100.00% ) in 0.61 seconds. BPR loss is 1.21E-04. Sample per second: 12927\n",
      "SLIM_BPR_Recommender: Epoch 94 of 100. Elapsed time 2.07 sec\n",
      "Processed 7947 ( 100.00% ) in 0.64 seconds. BPR loss is 1.23E-04. Sample per second: 12402\n",
      "SLIM_BPR_Recommender: Epoch 95 of 100. Elapsed time 2.10 sec\n",
      "Processed 7947 ( 100.00% ) in 0.67 seconds. BPR loss is 1.33E-04. Sample per second: 11927\n",
      "SLIM_BPR_Recommender: Epoch 96 of 100. Elapsed time 2.12 sec\n",
      "Processed 7947 ( 100.00% ) in 0.69 seconds. BPR loss is 1.34E-04. Sample per second: 11562\n",
      "SLIM_BPR_Recommender: Epoch 97 of 100. Elapsed time 2.14 sec\n",
      "Processed 7947 ( 100.00% ) in 0.71 seconds. BPR loss is 1.33E-04. Sample per second: 11240\n",
      "SLIM_BPR_Recommender: Epoch 98 of 100. Elapsed time 2.16 sec\n",
      "Processed 7947 ( 100.00% ) in 0.73 seconds. BPR loss is 1.32E-04. Sample per second: 10907\n",
      "SLIM_BPR_Recommender: Epoch 99 of 100. Elapsed time 2.19 sec\n",
      "Processed 7947 ( 100.00% ) in 0.75 seconds. BPR loss is 1.39E-04. Sample per second: 10576\n",
      "SLIM_BPR_Recommender: Epoch 100 of 100. Elapsed time 2.21 sec\n",
      "SLIM_BPR_Recommender: Terminating at epoch 100. Elapsed time 16.36 sec\n",
      "Deallocating Cython objects\n"
     ]
    }
   ],
   "source": [
    "from SLIM_BPR.Cython.SLIM_BPR_Cython import SLIM_BPR_Cython\n",
    "\n",
    "\n",
    "\n",
    "recommender = SLIM_BPR_Cython(URM_train, recompile_cython=False)\n",
    "\n",
    "recommender.fit(epochs=100, batch_size=1, sgd_mode='sgd', learning_rate=1e-4, positive_threshold_BPR=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Processed 5670 ( 100.00% ) in 3.30 sec. Users per second: 1720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({5: {'ROC_AUC': 0.10061728395061724,\n",
       "   'PRECISION': 0.038659611992944924,\n",
       "   'PRECISION_RECALL_MIN_DEN': 0.07601410934744214,\n",
       "   'RECALL': 0.06835178630285794,\n",
       "   'MAP': 0.04485273368606695,\n",
       "   'MRR': 0.09560258671369762,\n",
       "   'NDCG': 0.05878497690481431,\n",
       "   'F1': 0.0493863939650402,\n",
       "   'HIT_RATE': 0.19329805996472663,\n",
       "   'ARHR': 0.10441798941798945,\n",
       "   'NOVELTY': 0.0023576040703487153,\n",
       "   'AVERAGE_POPULARITY': 0.142431958791747,\n",
       "   'DIVERSITY_MEAN_INTER_LIST': 0.9833715653342865,\n",
       "   'DIVERSITY_HERFINDAHL': 0.9966396262391559,\n",
       "   'COVERAGE_ITEM': 0.2578248315688162,\n",
       "   'COVERAGE_ITEM_CORRECT': 0.01994225216554379,\n",
       "   'COVERAGE_USER': 0.7134767836919592,\n",
       "   'COVERAGE_USER_CORRECT': 0.11715112621114886,\n",
       "   'DIVERSITY_GINI': 0.08526591633356853,\n",
       "   'SHANNON_ENTROPY': 10.574770530321063}},\n",
       " 'CUTOFF: 5 - ROC_AUC: 0.1006173, PRECISION: 0.0386596, PRECISION_RECALL_MIN_DEN: 0.0760141, RECALL: 0.0683518, MAP: 0.0448527, MRR: 0.0956026, NDCG: 0.0587850, F1: 0.0493864, HIT_RATE: 0.1932981, ARHR: 0.1044180, NOVELTY: 0.0023576, AVERAGE_POPULARITY: 0.1424320, DIVERSITY_MEAN_INTER_LIST: 0.9833716, DIVERSITY_HERFINDAHL: 0.9966396, COVERAGE_ITEM: 0.2578248, COVERAGE_ITEM_CORRECT: 0.0199423, COVERAGE_USER: 0.7134768, COVERAGE_USER_CORRECT: 0.1171511, DIVERSITY_GINI: 0.0852659, SHANNON_ENTROPY: 10.5747705, \\n')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_test, cutoff_list=[5])\n",
    "\n",
    "\n",
    "evaluator_validation.evaluateRecommender(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: URM Detected 1079 (4.15 %) cold items.\n",
      "Deallocating Cython objects\n",
      "Unable to read memory status: list index out of range\n",
      "SLIM_BPR_Recommender: Automatic selection of fastest train mode. Unable to get current RAM status, you may be using a non-Linux operating system. Using dense matrix.\n",
      "Processed 7947 ( 100.00% ) in 0.92 seconds. BPR loss is 1.79E-08. Sample per second: 8651\n",
      "SLIM_BPR_Recommender: Epoch 1 of 10. Elapsed time 0.02 sec\n",
      "Processed 7947 ( 100.00% ) in 0.94 seconds. BPR loss is 1.05E-07. Sample per second: 8427\n",
      "SLIM_BPR_Recommender: Epoch 2 of 10. Elapsed time 0.05 sec\n",
      "Processed 7947 ( 100.00% ) in 0.97 seconds. BPR loss is 2.16E-07. Sample per second: 8193\n",
      "SLIM_BPR_Recommender: Epoch 3 of 10. Elapsed time 0.07 sec\n",
      "Processed 7947 ( 100.00% ) in 0.99 seconds. BPR loss is 4.14E-07. Sample per second: 7995\n",
      "SLIM_BPR_Recommender: Epoch 4 of 10. Elapsed time 0.10 sec\n",
      "Processed 7947 ( 100.00% ) in 1.02 seconds. BPR loss is 6.32E-07. Sample per second: 7810\n",
      "SLIM_BPR_Recommender: Epoch 5 of 10. Elapsed time 0.12 sec\n",
      "Processed 7947 ( 100.00% ) in 0.04 seconds. BPR loss is 9.16E-07. Sample per second: 194133\n",
      "SLIM_BPR_Recommender: Epoch 6 of 10. Elapsed time 0.14 sec\n",
      "Processed 7947 ( 100.00% ) in 0.07 seconds. BPR loss is 1.18E-06. Sample per second: 116449\n",
      "SLIM_BPR_Recommender: Epoch 7 of 10. Elapsed time 0.17 sec\n",
      "Processed 7947 ( 100.00% ) in 0.09 seconds. BPR loss is 1.57E-06. Sample per second: 86673\n",
      "SLIM_BPR_Recommender: Epoch 8 of 10. Elapsed time 0.19 sec\n",
      "Processed 7947 ( 100.00% ) in 0.12 seconds. BPR loss is 1.98E-06. Sample per second: 69073\n",
      "SLIM_BPR_Recommender: Epoch 9 of 10. Elapsed time 0.22 sec\n",
      "Processed 7947 ( 100.00% ) in 0.14 seconds. BPR loss is 2.50E-06. Sample per second: 57782\n",
      "SLIM_BPR_Recommender: Epoch 10 of 10. Elapsed time 0.24 sec\n",
      "SLIM_BPR_Recommender: Terminating at epoch 10. Elapsed time 13.18 sec\n",
      "Deallocating Cython objects\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIM_BPR_Cython(URM_all, recompile_cython=False)\n",
    "\n",
    "recommender.fit(epochs=10, batch_size=1, sgd_mode='sgd', learning_rate=1e-4, positive_threshold_BPR=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoms = recommender.recommend(userTestList, cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomList = []\n",
    "for i in range(len(recoms)):\n",
    "    recomList.append(' '.join(str(e) for e in recoms[i]))\n",
    "# print(recoms)\n",
    "\n",
    "res = {\"user_id\": userTestList, \"item_list\": recomList}\n",
    "result = pd.DataFrame(res, columns= ['user_id', 'item_list'])\n",
    "\n",
    "result.to_csv ('outputs/slim_brp1.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLIM ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import time, sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SLIMElasticNetRecommender(object):\n",
    "    \"\"\"\n",
    "    Train a Sparse Linear Methods (SLIM) item similarity model.\n",
    "    NOTE: ElasticNet solver is parallel, a single intance of SLIM_ElasticNet will\n",
    "          make use of half the cores available\n",
    "\n",
    "    See:\n",
    "        Efficient Top-N Recommendation by Linear Regression,\n",
    "        M. Levy and K. Jack, LSRS workshop at RecSys 2013.\n",
    "        https://www.slideshare.net/MarkLevy/efficient-slides\n",
    "\n",
    "        SLIM: Sparse linear methods for top-n recommender systems,\n",
    "        X. Ning and G. Karypis, ICDM 2011.\n",
    "        http://glaros.dtc.umn.edu/gkhome/fetch/papers/SLIM2011icdm.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"SLIMElasticNetRecommender\"\n",
    "\n",
    "    def __init__(self, URM_train):\n",
    "\n",
    "        super(SLIMElasticNetRecommender, self).__init__()\n",
    "\n",
    "        self.URM_train = URM_train\n",
    "\n",
    "\n",
    "    def fit(self, l1_penalty=0.1, l2_penalty=0.1, positive_only=True, topK = 100):\n",
    "\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.positive_only = positive_only\n",
    "        self.topK = topK\n",
    "\n",
    "        if self.l1_penalty + self.l2_penalty != 0:\n",
    "            self.l1_ratio = self.l1_penalty / (self.l1_penalty + self.l2_penalty)\n",
    "        else:\n",
    "            print(\"SLIM_ElasticNet: l1_penalty+l2_penalty cannot be equal to zero, setting the ratio l1/(l1+l2) to 1.0\")\n",
    "            self.l1_ratio = 1.0\n",
    "\n",
    "\n",
    "\n",
    "        # initialize the ElasticNet model\n",
    "        self.model = ElasticNet(alpha=1.0,\n",
    "                                l1_ratio=self.l1_ratio,\n",
    "                                positive=self.positive_only,\n",
    "                                fit_intercept=False,\n",
    "                                copy_X=False,\n",
    "                                precompute=True,\n",
    "                                selection='random',\n",
    "                                max_iter=100,\n",
    "                                tol=1e-4)\n",
    "\n",
    "\n",
    "        URM_train = sps.csc_matrix(self.URM_train)\n",
    "\n",
    "        n_items = URM_train.shape[1]\n",
    "\n",
    "\n",
    "        # Use array as it reduces memory requirements compared to lists\n",
    "        dataBlock = 10000000\n",
    "\n",
    "        rows = np.zeros(dataBlock, dtype=np.int32)\n",
    "        cols = np.zeros(dataBlock, dtype=np.int32)\n",
    "        values = np.zeros(dataBlock, dtype=np.float32)\n",
    "\n",
    "        numCells = 0\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_printBatch = start_time\n",
    "\n",
    "        # fit each item's factors sequentially (not in parallel)\n",
    "        for currentItem in range(n_items):\n",
    "\n",
    "            # get the target column\n",
    "            y = URM_train[:, currentItem].toarray()\n",
    "\n",
    "            # set the j-th column of X to zero\n",
    "            start_pos = URM_train.indptr[currentItem]\n",
    "            end_pos = URM_train.indptr[currentItem + 1]\n",
    "\n",
    "            current_item_data_backup = URM_train.data[start_pos: end_pos].copy()\n",
    "            URM_train.data[start_pos: end_pos] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "            # fit one ElasticNet model per column\n",
    "            self.model.fit(URM_train, y)\n",
    "\n",
    "            # self.model.coef_ contains the coefficient of the ElasticNet model\n",
    "            # let's keep only the non-zero values\n",
    "\n",
    "            # Select topK values\n",
    "            # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "            # - Partition the data to extract the set of relevant items\n",
    "            # - Sort only the relevant items\n",
    "            # - Get the original item index\n",
    "\n",
    "            nonzero_model_coef_index = self.model.sparse_coef_.indices\n",
    "            nonzero_model_coef_value = self.model.sparse_coef_.data\n",
    "\n",
    "\n",
    "            local_topK = min(len(nonzero_model_coef_value)-1, self.topK)\n",
    "\n",
    "            relevant_items_partition = (-nonzero_model_coef_value).argpartition(local_topK)[0:local_topK]\n",
    "            relevant_items_partition_sorting = np.argsort(-nonzero_model_coef_value[relevant_items_partition])\n",
    "            ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "\n",
    "            for index in range(len(ranking)):\n",
    "\n",
    "                if numCells == len(rows):\n",
    "                    rows = np.concatenate((rows, np.zeros(dataBlock, dtype=np.int32)))\n",
    "                    cols = np.concatenate((cols, np.zeros(dataBlock, dtype=np.int32)))\n",
    "                    values = np.concatenate((values, np.zeros(dataBlock, dtype=np.float32)))\n",
    "\n",
    "\n",
    "                rows[numCells] = nonzero_model_coef_index[ranking[index]]\n",
    "                cols[numCells] = currentItem\n",
    "                values[numCells] = nonzero_model_coef_value[ranking[index]]\n",
    "\n",
    "                numCells += 1\n",
    "\n",
    "\n",
    "            # finally, replace the original values of the j-th column\n",
    "            URM_train.data[start_pos:end_pos] = current_item_data_backup\n",
    "\n",
    "\n",
    "            if time.time() - start_time_printBatch > 300 or currentItem == n_items-1:\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} minutes. Items per second: {:.0f}\".format(\n",
    "                                  currentItem+1,\n",
    "                                  100.0* float(currentItem+1)/n_items,\n",
    "                                  (time.time()-start_time)/60,\n",
    "                                  float(currentItem)/(time.time()-start_time)))\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_printBatch = time.time()\n",
    "\n",
    "\n",
    "        # generate the sparse weight matrix\n",
    "        self.W_sparse = sps.csr_matrix((values[:numCells], (rows[:numCells], cols[:numCells])),\n",
    "                                       shape=(n_items, n_items), dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM_train[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM_train.indptr[user_id]\n",
    "        end_pos = self.URM_train.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM_train.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deallocating Cython objects\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIMElasticNetRecommender(URM_train)\n",
    "\n",
    "recommender.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_algorithm(URM_test, recommender)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}